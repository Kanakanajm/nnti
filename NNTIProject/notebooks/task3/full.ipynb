{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMlj6ZSouPQNdtjnBgZUpQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanakanajm/nnti/blob/main/nnti/NNTIProject%20/notebooks/task3-full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4CykTBdvEzc"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install datasets torch transformers[torch] # wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wandb login"
      ],
      "metadata": {
        "id": "gLoItGYhl05f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# libs\n",
        "import os\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from torch.cuda import empty_cache as cuda_empty_cache, mem_get_info\n",
        "from gc import collect as garbage_collect\n",
        "\n",
        "# consts\n",
        "MODEL_NAME = \"facebook/xglm-564M\"\n",
        "CACHE_DIR_DATASETS = \"cache/datasets\"\n",
        "CACHE_DIR_TOKENIZERS = \"cache/tokenizers\"\n",
        "CACHE_DIR_MODELS = \"cache/models\"\n",
        "\n",
        "# env vars\n",
        "# set the wandb project where this run will be logged\n",
        "os.environ[\"WANDB_PROJECT\"]=\"xglm-full\"\n",
        "\n",
        "# save your trained model checkpoint to wandb\n",
        "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
        "\n",
        "# turn off watch to log faster\n",
        "os.environ[\"WANDB_WATCH\"]=\"false\""
      ],
      "metadata": {
        "id": "6cKjYD9XhrMe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer init\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR_TOKENIZERS)"
      ],
      "metadata": {
        "id": "8DBlo6hPmHmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper funcs\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": np.mean(predictions == labels)}\n",
        "\n",
        "def clean():\n",
        "    # release memory\n",
        "    garbage_collect()\n",
        "    cuda_empty_cache()\n",
        "\n",
        "    mem_info = mem_get_info()\n",
        "    print(f\"Freeing GPU Memory\\nFree: %d MB\\tTotal: %d MB\" % (mem_info[0] // 1024**2, mem_info[1] // 1024**2))\n",
        "\n",
        "# set padding token to -100 in labels\n",
        "def to_label_id(id):\n",
        "    if (id == tokenizer.pad_token_id):\n",
        "        return -100\n",
        "    return id\n",
        "\n",
        "# preprocess sentence into length 16 token chunks (w/padding)\n",
        "def preprocess(batch):\n",
        "    result = tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=16\n",
        "        # return_overflowing_tokens=True,\n",
        "    )\n",
        "    result['labels'] = list(map(to_label_id, result['input_ids']))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "GmdApUgvEk1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(dataset):\n",
        "    return dataset.remove_columns('text').with_format('torch')\n",
        "\n",
        "def load_task3_datasets():\n",
        "    train_dataset = load_dataset(\"Llamacha/monolingual-quechua-iic\", split=\"train\", cache_dir=CACHE_DIR_DATASETS)\n",
        "    test_dataset = load_dataset(\"facebook/flores\", \"quy_Latn\", split=\"devtest\", cache_dir=CACHE_DIR_DATASETS).remove_columns(['id', 'URL', 'domain', 'topic', 'has_image', 'has_hyperlink']).rename_column(\"sentence\", \"text\")\n",
        "\n",
        "    # try a smaller dataset\n",
        "    train_dataset = train_dataset.select(range(16384))\n",
        "    # test_dataset = test_dataset.select(range(128))\n",
        "\n",
        "    # tokenize\n",
        "    # no dynamic padding\n",
        "    tokenized_train_dataset = postprocess(train_dataset.map(preprocess, batched=True))\n",
        "    tokenized_test_dataset = postprocess(test_dataset.map(preprocess, batched=True))\n",
        "\n",
        "    del train_dataset, test_dataset\n",
        "    return tokenized_train_dataset, tokenized_test_dataset\n"
      ],
      "metadata": {
        "id": "2ZEKZk1nyqTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "#train_dataloader = DataLoader(tokenized_dataset['train'], batch_size=16, shuffle=False)\n",
        "#test_dataloader = DataLoader(tokenized_dataset['test'], batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "IGS7ZJqZ0ZOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# consts for training\n",
        "DEFAULT_TRAIN_ARGS = TrainingArguments(\n",
        "    output_dir='models',\n",
        "    # report_to=\"wandb\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # do_eval=False\n",
        "    push_to_hub=False,\n",
        "    # logging_steps=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    # eval_accumulation_steps = 20,\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # eval_steps=20,\n",
        "    # max_steps = 100,\n",
        "    # save_steps = 100,\n",
        "    save_total_limit = 2,\n",
        ")\n",
        "\n",
        "TRAIN_DATASET, TEST_DATASET = load_task3_datasets()\n",
        "\n",
        "def get_default_trainer(model):\n",
        "    return Trainer(\n",
        "        model=model,\n",
        "        args=DEFAULT_TRAIN_ARGS,\n",
        "        train_dataset=TRAIN_DATASET,\n",
        "        eval_dataset=TEST_DATASET,\n",
        "        # compute_metrics=compute_metrics,\n",
        "    )\n",
        "def get_default_model():\n",
        "    return AutoModelForCausalLM.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR_MODELS).to(\"cuda\")"
      ],
      "metadata": {
        "id": "W0P7J273nEMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full fine tune"
      ],
      "metadata": {
        "id": "CzgJHgrPmnfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    # adaptation (full fine tune)\n",
        "    model = get_default_model()\n",
        "    get_default_trainer(model).train(\n",
        "        # resume_from_checkpoint = True\n",
        "        )\n"
      ],
      "metadata": {
        "id": "nOrdcd8w1uSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean()\n",
        "train_model()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "IVqOV4JNKv_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}