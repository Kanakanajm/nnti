{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Language model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this first task is to familiarize yourself with the huggingface transformers and dataset libraries. You will learn how to load and tokenize a dataset, how to load a pre-trained language model, and finally, how to run a model in inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_dataset_builder,\n",
    "    get_dataset_split_names,\n",
    "    get_dataset_config_names,\n",
    ")\n",
    "from transformers import (\n",
    "    XGLMTokenizer,\n",
    "    XGLMTokenizerFast,\n",
    "    XGLMForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# set up figure parameters to make them look nice\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "matplotlib.rcParams[\"font.family\"] = \"cmr10\"\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "matplotlib.rcParams.update({\"font.size\": 11})\n",
    "\n",
    "# other utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\" # specify dataset name\n",
    "MODEL_NAME = \"facebook/xglm-564M\" # specify model name\n",
    "# MODEL_NAME = \"gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creation of FLORES-200 doubles the existing language coverage of FLORES-101. \n",
      "Given the nature of the new languages, which have less standardization and require \n",
      "more specialized professional translations, the verification process became more complex. \n",
      "This required modifications to the translation workflow. FLORES-200 has several languages \n",
      "which were not translated from English. Specifically, several languages were translated \n",
      "from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also \n",
      "includes two script alternatives for four languages. FLORES-200 consists of translations \n",
      "from 842 distinct web articles, totaling 3001 sentences. These sentences are divided \n",
      "into three splits: dev, devtest, and test (hidden). On average, sentences are approximately \n",
      "21 words long.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore a dataset\n",
    "LANGUAGE_CODE = \"deu_Latn\" # Language to explore\n",
    "\n",
    "# covered language codes can be found here: https://github.com/openlanguagedata/flores?tab=readme-ov-file#language-coverage\n",
    "\n",
    "ds_builder = load_dataset_builder(DATA_SET_NAME, LANGUAGE_CODE, trust_remote_code=True)\n",
    "print(ds_builder.info.description) # print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'URL': Value(dtype='string', id=None),\n",
      " 'domain': Value(dtype='string', id=None),\n",
      " 'has_hyperlink': Value(dtype='int32', id=None),\n",
      " 'has_image': Value(dtype='int32', id=None),\n",
      " 'id': Value(dtype='int32', id=None),\n",
      " 'sentence': Value(dtype='string', id=None),\n",
      " 'topic': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# print the features (columns) of the dataset\n",
    "pprint(ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# get the available splits\n",
    "pprint(ds_builder.info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, tokenize, and batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify languages\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for eng_Latn... done\n",
      "Loading dataset for spa_Latn... done\n",
      "Loading dataset for ita_Latn... done\n",
      "Loading dataset for deu_Latn... done\n",
      "Loading dataset for arb_Arab... done\n",
      "Loading dataset for tel_Telu... done\n",
      "Loading dataset for tam_Taml... done\n",
      "Loading dataset for quy_Latn... done\n"
     ]
    }
   ],
   "source": [
    "# Set up the splits to download\n",
    "USE_SPLITS = [\"dev\", \"devtest\"]\n",
    "\n",
    "\"\"\"\n",
    "load flores data for each language\n",
    "structure: \n",
    "dataset_per_lang = {\n",
    "  language: {\n",
    "      \"dataset\": {\n",
    "           split (dev/devtest): {\n",
    "               \"raw\": raw dataset (without tokenization),\n",
    "               \"tokenized\": tokenized dataset\n",
    "           }\n",
    "      }, \n",
    "      \"dataloader\": None}\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "dataset_per_lang = {}\n",
    "for language in LANGUAGES:\n",
    "    print(f\"Loading dataset for {language}\", end=\"... \")\n",
    "\n",
    "    # add a dataloader key set to None, they are defined in the cell tagged\n",
    "    # @dataloader-creation\n",
    "    dataset_per_lang[language] = {\"dataset\": {}, \"dataloader\": None}\n",
    "\n",
    "    for split in USE_SPLITS:\n",
    "        dataset_per_lang[language][\"dataset\"][split] = {}\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"raw\"] = load_dataset(\n",
    "            DATA_SET_NAME,\n",
    "            language,\n",
    "            split=split,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=\"../cache/languages\",\n",
    "        )\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the english dataset: 501481\n",
      "Features:\n",
      "\t{'URL': Value(dtype='string', id=None),\n",
      "\t 'domain': Value(dtype='string', id=None),\n",
      "\t 'has_hyperlink': Value(dtype='int32', id=None),\n",
      "\t 'has_image': Value(dtype='int32', id=None),\n",
      "\t 'id': Value(dtype='int32', id=None),\n",
      "\t 'sentence': Value(dtype='string', id=None),\n",
      "\t 'topic': Value(dtype='string', id=None)}\n",
      "\n",
      "Splits:\n",
      "{'dev': SplitInfo(name='dev', num_bytes=245488, num_examples=997, shard_lengths=None, dataset_name='flores'), 'devtest': SplitInfo(name='devtest', num_bytes=255993, num_examples=1012, shard_lengths=None, dataset_name='flores')}\n"
     ]
    }
   ],
   "source": [
    "# let's look at the English subset\n",
    "EX_DATASET_LANG = \"eng_Latn\"\n",
    "english_dataset = dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"]\n",
    "print(f\"Size of the english dataset: {english_dataset.info.dataset_size}\")\n",
    "print(\"Features:\")\n",
    "pprint_tab(english_dataset.info.features)\n",
    "print(\"\\nSplits:\")\n",
    "print(english_dataset.info.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing raw samples from eng_Latn:\n",
      "\n",
      "\tFirst sample from dev split:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1,\n",
      "\t\t 'sentence': 'On Monday, scientists from the Stanford University School of '\n",
      "\t\t             'Medicine announced the invention of a new diagnostic tool that '\n",
      "\t\t             'can sort cells by type: a tiny printable chip that can be '\n",
      "\t\t             'manufactured using standard inkjet printers for possibly about '\n",
      "\t\t             'one U.S. cent each.',\n",
      "\t\t 'topic': 'health'}\n",
      "\n",
      "\t996-th sample from dev split:\n",
      "\t\t{'URL': 'https://en.wikivoyage.org/wiki/Funeral_travel',\n",
      "\t\t 'domain': 'wikivoyage',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 997,\n",
      "\t\t 'sentence': 'In all cases, you must book by phone directly with the airline.',\n",
      "\t\t 'topic': 'Reason to travel/Funeral travel'}\n",
      "\n",
      "\tFirst sample from devtest split:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Toronto_team-led_research_on_Type_1_Diabetes_%27groundbreaking%27',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1,\n",
      "\t\t 'sentence': '\"We now have 4-month-old mice that are non-diabetic that used to '\n",
      "\t\t             'be diabetic,\" he added.',\n",
      "\t\t 'topic': 'disease, research, canada'}\n",
      "\n",
      "\t1011-th sample from devtest split:\n",
      "\t\t{'URL': 'https://en.wikivoyage.org/wiki/Working_and_studying_in_Japan',\n",
      "\t\t 'domain': 'wikivoyage',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1012,\n",
      "\t\t 'sentence': \"Workers must often get their superiors' approval for any \"\n",
      "\t\t             \"decisions they make, and are expected to obey their superiors' \"\n",
      "\t\t             'instructions without question.',\n",
      "\t\t 'topic': 'Reason to travel/Working in Japan'}\n"
     ]
    }
   ],
   "source": [
    "# let's look at an individual sample from the dataset\n",
    "def get_sample(idx: int, lang: str, split: str, data: str):\n",
    "    return dataset_per_lang[lang]['dataset'][split][data][idx]\n",
    "\n",
    "print(f\"Viewing raw samples from {EX_DATASET_LANG}:\")\n",
    "for split in USE_SPLITS:\n",
    "    first_sample = get_sample(0, EX_DATASET_LANG, split, \"raw\")\n",
    "    last_sample = get_sample(-1, EX_DATASET_LANG, split, \"raw\")\n",
    "    dataset_len = len(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][split][\"raw\"]) - 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"\\tFirst sample from {split} split:\")\n",
    "    pprint_tab(first_sample, indent=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    print(f\"\\t{dataset_len}-th sample from {split} split:\")\n",
    "    pprint_tab(last_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493d4fe0728f4c588ea8dc6c94730575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ab1548e38c4898ad1e9c89c304b743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "# if this throws an error and one is using a conda environment, one has to\n",
    "# install sentencepiece library: conda install -c huggingface sentencepiece\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME, cache_dir=\"../cache/tokenizers\"\n",
    "    )\n",
    "except ValueError:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME, cache_dir=\"../cache/tokenizers\", use_fast=False\n",
    "    )\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.unk_token})\n",
    "\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_batch_dimension(example):\n",
    "    \"\"\"\n",
    "    Adds a batch dimension to the tensors.\n",
    "    This function assumes the tensors are already in PyTorch tensors and simply\n",
    "    unsqueezes them at the first dimension.\n",
    "    See https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "    \"\"\"\n",
    "    example[\"input_ids\"] = example[\"input_ids\"].unsqueeze(0)\n",
    "    example[\"attention_mask\"] = example[\"attention_mask\"].unsqueeze(0)\n",
    "    return example\n",
    "\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        raw_dataset = copy(dataset_per_lang[language][\"dataset\"][split][\"raw\"])\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = raw_dataset.map(\n",
    "            lambda example: tokenization(example), batched=True\n",
    "        )\n",
    "\n",
    "        # Update the dataset with Pytorch format\n",
    "        tokenized_dataset.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        # Apply unsqueeze operation\n",
    "        tokenized_dataset = tokenized_dataset.map(\n",
    "            lambda example: add_batch_dimension(example),\n",
    "            batched=False,  # Set batched=False to apply function to each example individually\n",
    "        )\n",
    "\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"tokenized\"] = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing 17-th sample from eng_Latn:\n",
      "\tRaw sample:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Investigation_of_Deutsche_Bank_headquarters_spills_into_second_day',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 18,\n",
      "\t\t 'sentence': 'British newspaper The Guardian suggested Deutsche Bank '\n",
      "\t\t             'controlled roughly a third of the 1200 shell companies used to '\n",
      "\t\t             'accomplish this.',\n",
      "\t\t 'topic': 'crime'}\n",
      "\n",
      "\tTokenized sample:\n",
      "\t\t{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "\t\t         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "\t\t         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "\t\t         0]]),\n",
      "\t\t 'input_ids': tensor([[     2,  23409, 123980,    268,  67521, 102943,  22532,   5355, 170318,\n",
      "\t\t              6, 208717,     11,  27643,     48,     32,  27933, 105094,  33409,\n",
      "\t\t           3964,     33, 169662,    319,      5,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1]])}\n",
      "\n",
      "Viewing 17-th sample from spa_Latn:\n",
      "\tRaw sample:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Investigation_of_Deutsche_Bank_headquarters_spills_into_second_day',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 18,\n",
      "\t\t 'sentence': 'El periódico británico The Guardian señaló que Deutsche Bank '\n",
      "\t\t             'tenía el control de cerca de la tercera parte de las 1200 '\n",
      "\t\t             'compañías fantasma que se usaron para llevar esto a cabo.',\n",
      "\t\t 'topic': 'crime'}\n",
      "\n",
      "\tTokenized sample:\n",
      "\t\t{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "\t\t         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "\t\t         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "\t\t         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      "\t\t 'input_ids': tensor([[     2,    453, 190847, 200580,     19,    268,  67521, 167160,     52,\n",
      "\t\t          22532,   5355,  39693,     85,   2867,     10,   9431,     10,     24,\n",
      "\t\t          82881,   1646,     10,    504,  27933, 218690, 190560,     52,     41,\n",
      "\t\t          18207,    231,    142,  31694,  11050,     11,  35787,      5,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t              1,      1,      1]])}\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "LOOKAT_SAMPLE_ID = 17\n",
    "\n",
    "# get raw and tokenized sample\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")\n",
    "\n",
    "print(f\"Viewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")\n",
    "\n",
    "EX_DATASET_LANG = \"spa_Latn\"\n",
    "\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")\n",
    "\n",
    "print(f\"\\nViewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "dataloader-creation"
    ]
   },
   "outputs": [],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "BATCH_SIZE = 2  # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        tokenized_dataset = dataset_per_lang[language][\"dataset\"][split][\"tokenized\"]\n",
    "        dataset_per_lang[language][\"dataloader\"] = torch.utils.data.DataLoader(\n",
    "            tokenized_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model from the huggingface hub\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, cache_dir=\"../cache/models\")\n",
    "\n",
    "# specify device on model and put the model into evaluation mode\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.76503562927246\n"
     ]
    }
   ],
   "source": [
    "# test on a sample\n",
    "inputs = tokenized_sample[\"input_ids\"].to(device)\n",
    "labels = tokenized_sample[\"input_ids\"].to(device)\n",
    "\n",
    "# torch.inference_mode() is now preferred over torch.no_grad().\n",
    "# See: https://discuss.pytorch.org/t/pytorch-torch-no-grad-vs-torch-inference-mode/134099/2?u=timgianitsos\n",
    "with torch.inference_mode():\n",
    "    outputs = model(inputs, labels=labels, attention_mask=tokenized_sample[\"attention_mask\"].to(device))\n",
    "    loss = outputs.loss.item()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing losses for eng_Latn... \n",
      "\tSplit: dev... "
     ]
    }
   ],
   "source": [
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# Frees unused memory so it can be used by other tensors\n",
    "torch.cuda.empty_cache()  \n",
    "del inputs, labels, outputs\n",
    "\n",
    "# TODO: Revise this to make sure it uses the cross-entropy loss\n",
    "# iterate over the dataset for each language and compute the cross-entropy loss per batch \n",
    "for language in dataset_per_lang:\n",
    "    print(f\"Computing losses for {language}\", end=\"... \\n\")\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        print(f\"\\tSplit: {split}\", end=\"... \")\n",
    "        dataloader = dataset_per_lang[language][\"dataloader\"]\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # torch.inference_mode() is now preferred over torch.no_grad().\n",
    "            # See: https://discuss.pytorch.org/t/pytorch-torch-no-grad-vs-torch-inference-mode/134099/2?u=timgianitsos\n",
    "            with torch.inference_mode():\n",
    "                outputs = model(inputs, labels=labels, attention_mask=attention_mask)\n",
    "                loss = outputs.loss.item()\n",
    "            \n",
    "            losses[language].append(loss)\n",
    "\n",
    "            # Explicitly delete tensors to free up GPU memory\n",
    "            del inputs, labels, attention_mask, outputs\n",
    "\n",
    "        print(\"done\")\n",
    "\n",
    "    # After processing each language, try to free up memory explicitly\n",
    "    torch.cuda.empty_cache()  # Frees unused memory so it can be used by other tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# create a bar plot for each language\n",
    "x = np.arange(len(LANGUAGES))\n",
    "y = [np.mean(losses[\"eng_Latn\"][1]) for language in LANGUAGES]\n",
    "\n",
    "axes.bar(x, y)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# format plot\n",
    "axes.set_xlabel(\"Language\") # x-axis label\n",
    "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
    "axes.set_xticklabels(losses.keys()) # x-axis tick labels\n",
    "axes.set_ylabel(\"Loss\") # y-axis label\n",
    "axes.set_ylim(0, 9) # range of y-axis\n",
    "axes.set_title(MODEL_NAME); # title\n",
    "axes.grid(True, which='major', color='k', linestyle='-', alpha=0.2)\n",
    "axes.grid(True, which='minor', color='k', linestyle='--', alpha=0.1)\n",
    "axes.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing XGLM to GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to re-run the analysis above, but using `gpt2` as the pre-trained language model. For this exercise, focus on your native language, unless it's English or isn't covered by flores. In that case, pick another language that you can read well. \n",
    "\n",
    "Compare the language modeling loss of XGLM and GPT2. What do you observe? Investigate the differences in tokenization for XGLM and GPT2. What do you observe? How can the good (or bad) performance of GPT2 be explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
