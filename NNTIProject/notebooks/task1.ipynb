{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Language model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this first task is to familiarize yourself with the huggingface transformers and dataset libraries. You will learn how to load and tokenize a dataset, how to load a pre-trained language model, and finally, how to run a model in inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_dataset_builder,\n",
    "    get_dataset_split_names,\n",
    "    get_dataset_config_names,\n",
    ")\n",
    "from transformers import (\n",
    "    XGLMTokenizer,\n",
    "    XGLMTokenizerFast,\n",
    "    XGLMForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# set up figure parameters to make them look nice\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "matplotlib.rcParams[\"font.family\"] = \"cmr10\"\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "matplotlib.rcParams.update({\"font.size\": 11})\n",
    "\n",
    "# other utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\" # specify dataset name\n",
    "MODEL_NAME = \"facebook/xglm-564M\" # specify model name\n",
    "# MODEL_NAME = \"gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a dataset\n",
    "LANGUAGE_CODE = \"deu_Latn\" # Language to explore\n",
    "\n",
    "# covered language codes can be found here: https://github.com/openlanguagedata/flores?tab=readme-ov-file#language-coverage\n",
    "\n",
    "ds_builder = load_dataset_builder(DATA_SET_NAME, LANGUAGE_CODE, trust_remote_code=True)\n",
    "print(ds_builder.info.description) # print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the features (columns) of the dataset\n",
    "print(ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the available splits\n",
    "print(ds_builder.info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, tokenize, and batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify languages\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the splits to download\n",
    "USE_SPLITS = [\"dev\", \"devtest\"]\n",
    "\n",
    "# load flores data for each language\n",
    "# structure: \n",
    "# dataset_per_lang = {\n",
    "#   language: {\n",
    "#       \"dataset\": {\n",
    "#            split (dev/devtest): {\n",
    "#                \"raw\": raw dataset (without tokenization),\n",
    "#                \"tokenized\": tokenized dataset\n",
    "#            }\n",
    "#       }, \n",
    "#       \"dataloader\": None}\n",
    "#   }\n",
    "# }\n",
    "dataset_per_lang = {}\n",
    "for language in LANGUAGES:\n",
    "    print(f\"Loading dataset for {language}\", end=\"... \")\n",
    "\n",
    "    # add a dataloader key set to None, they are defined in the cell tagged\n",
    "    # @dataloader-creation\n",
    "    dataset_per_lang[language] = {\"dataset\": {}, \"dataloader\": None}\n",
    "\n",
    "    for split in USE_SPLITS:\n",
    "        dataset_per_lang[language][\"dataset\"][split] = {}\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"raw\"] = load_dataset(\n",
    "            DATA_SET_NAME,\n",
    "            language,\n",
    "            split=split,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=\"../cache/languages\",\n",
    "        )\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the English subset\n",
    "EX_DATASET_LANG = \"eng_Latn\"\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.dataset_size)\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.features)\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at an individual sample from the dataset\n",
    "def get_sample(idx: int, lang: str, split: str, data: str):\n",
    "    return dataset_per_lang[lang]['dataset'][split][data][idx]\n",
    "\n",
    "print(f\"Viewing raw samples from {EX_DATASET_LANG}:\")\n",
    "for split in USE_SPLITS:\n",
    "    first_sample = get_sample(0, EX_DATASET_LANG, split, \"raw\")\n",
    "    last_sample = get_sample(-1, EX_DATASET_LANG, split, \"raw\")\n",
    "    dataset_len = len(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][split][\"raw\"]) - 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"\\tFirst sample from {split} split:\")\n",
    "    pprint_tab(first_sample, indent=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    print(f\"\\t{dataset_len}-th sample from {split} split:\")\n",
    "    pprint_tab(last_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=\"../cache/tokenizers\")\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.unk_token})\n",
    "\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_batch_dimension(example):\n",
    "    \"\"\"\n",
    "    Adds a batch dimension to the tensors.\n",
    "    This function assumes the tensors are already in PyTorch tensors and simply\n",
    "    unsqueezes them at the first dimension.\n",
    "    See https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "    \"\"\"\n",
    "    example[\"input_ids\"] = example[\"input_ids\"].unsqueeze(0)\n",
    "    example[\"attention_mask\"] = example[\"attention_mask\"].unsqueeze(0)\n",
    "    return example\n",
    "\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        raw_dataset = copy(dataset_per_lang[language][\"dataset\"][split][\"raw\"])\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = raw_dataset.map(\n",
    "            lambda example: tokenization(example), batched=True\n",
    "        )\n",
    "\n",
    "        # Update the dataset with Pytorch format\n",
    "        tokenized_dataset.set_format(\n",
    "            type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        # Apply unsqueeze operation\n",
    "        tokenized_dataset = tokenized_dataset.map(\n",
    "            lambda example: add_batch_dimension(example),\n",
    "            batched=False,  # Set batched=False to apply function to each example individually\n",
    "        )\n",
    "\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"tokenized\"] = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "LOOKAT_SAMPLE_ID = 17\n",
    "\n",
    "# get raw and tokenized sample\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")\n",
    "\n",
    "print(f\"Viewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")\n",
    "\n",
    "EX_DATASET_LANG = \"spa_Latn\"\n",
    "\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")\n",
    "\n",
    "print(f\"\\nViewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dataloader-creation"
    ]
   },
   "outputs": [],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "BATCH_SIZE = 2  # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        tokenized_dataset = dataset_per_lang[language][\"dataset\"][split][\"tokenized\"]\n",
    "        dataset_per_lang[language][\"dataloader\"] = torch.utils.data.DataLoader(\n",
    "            tokenized_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model from the huggingface hub\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, cache_dir=\"../cache/models\")\n",
    "\n",
    "# specify device on model and put the model into evaluation mode\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(**tokenized_sample)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.logits.size() # should be (1, seq_len, vocab_size)\n",
    "preds.logits[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# TODO:\n",
    "# iterate over the dataset for each language and compute the cross-entropy loss per batch \n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        dataloader = dataset_per_lang[language][\"dataloader\"]\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"input_ids\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss.item()\n",
    "            \n",
    "            losses[language].append(loss)\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        dataloader = dataset_per_lang[language][\"dataloader\"]\n",
    "        for batch in dataloader:\n",
    "            # TODO: compute cross-entropy loss per batch\n",
    "            # loss = ...\n",
    "            # losses[language].append(loss)\n",
    "            pass\n",
    "\n",
    "losses[\"eng_Latn\"].append(np.linspace(0, 1, 10))\n",
    "losses[\"eng_Latn\"].append(np.linspace(1, 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# create a bar plot for each language\n",
    "x = np.arange(len(LANGUAGES))\n",
    "y = [np.mean(losses[\"eng_Latn\"][1]) for language in LANGUAGES]\n",
    "\n",
    "axes.bar(x, y)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# format plot\n",
    "axes.set_xlabel(\"Language\") # x-axis label\n",
    "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
    "axes.set_xticklabels(losses.keys()) # x-axis tick labels\n",
    "axes.set_ylabel(\"Loss\") # y-axis label\n",
    "axes.set_ylim(0, 9) # range of y-axis\n",
    "axes.set_title(MODEL_NAME); # title\n",
    "axes.grid(True, which='major', color='k', linestyle='-', alpha=0.2)\n",
    "axes.grid(True, which='minor', color='k', linestyle='--', alpha=0.1)\n",
    "axes.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing XGLM to GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to re-run the analysis above, but using `gpt2` as the pre-trained language model. For this exercise, focus on your native language, unless it's English or isn't covered by flores. In that case, pick another language that you can read well. \n",
    "\n",
    "Compare the language modeling loss of XGLM and GPT2. What do you observe? Investigate the differences in tokenization for XGLM and GPT2. What do you observe? How can the good (or bad) performance of GPT2 be explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
