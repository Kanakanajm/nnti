{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Language model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this first task is to familiarize yourself with the huggingface transformers and dataset libraries. You will learn how to load and tokenize a dataset, how to load a pre-trained language model, and finally, how to run a model in inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_dataset_builder,\n",
    "    get_dataset_split_names,\n",
    "    get_dataset_config_names,\n",
    ")\n",
    "from transformers import (\n",
    "    XGLMTokenizer,\n",
    "    XGLMTokenizerFast,\n",
    "    XGLMForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# set up figure parameters to make them look nice\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "matplotlib.rcParams[\"font.family\"] = \"cmr10\"\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "matplotlib.rcParams.update({\"font.size\": 11})\n",
    "\n",
    "# other utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\" # specify dataset name\n",
    "MODEL_NAME = \"facebook/xglm-564M\" # specify model name\n",
    "# MODEL_NAME = \"gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creation of FLORES-200 doubles the existing language coverage of FLORES-101. \n",
      "Given the nature of the new languages, which have less standardization and require \n",
      "more specialized professional translations, the verification process became more complex. \n",
      "This required modifications to the translation workflow. FLORES-200 has several languages \n",
      "which were not translated from English. Specifically, several languages were translated \n",
      "from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also \n",
      "includes two script alternatives for four languages. FLORES-200 consists of translations \n",
      "from 842 distinct web articles, totaling 3001 sentences. These sentences are divided \n",
      "into three splits: dev, devtest, and test (hidden). On average, sentences are approximately \n",
      "21 words long.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore a dataset\n",
    "LANGUAGE_CODE = \"deu_Latn\" # Language to explore\n",
    "\n",
    "# covered language codes can be found here: https://github.com/openlanguagedata/flores?tab=readme-ov-file#language-coverage\n",
    "\n",
    "ds_builder = load_dataset_builder(DATA_SET_NAME, LANGUAGE_CODE, trust_remote_code=True)\n",
    "print(ds_builder.info.description) # print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int32', id=None), 'URL': Value(dtype='string', id=None), 'domain': Value(dtype='string', id=None), 'topic': Value(dtype='string', id=None), 'has_image': Value(dtype='int32', id=None), 'has_hyperlink': Value(dtype='int32', id=None), 'sentence': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# print the features (columns) of the dataset\n",
    "print(ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev': SplitInfo(name='dev', num_bytes=269134, num_examples=997, shard_lengths=None, dataset_name='flores'), 'devtest': SplitInfo(name='devtest', num_bytes=280255, num_examples=1012, shard_lengths=None, dataset_name='flores')}\n"
     ]
    }
   ],
   "source": [
    "# get the available splits\n",
    "print(ds_builder.info.splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, tokenize, and batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify languages\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for eng_Latn... done\n",
      "Loading dataset for spa_Latn... done\n",
      "Loading dataset for ita_Latn... done\n",
      "Loading dataset for deu_Latn... done\n",
      "Loading dataset for arb_Arab... done\n",
      "Loading dataset for tel_Telu... done\n",
      "Loading dataset for tam_Taml... done\n",
      "Loading dataset for quy_Latn... done\n"
     ]
    }
   ],
   "source": [
    "# Set up the splits to download\n",
    "USE_SPLITS = [\"dev\", \"devtest\"]\n",
    "\n",
    "# load flores data for each language\n",
    "# structure: \n",
    "# dataset_per_lang = {\n",
    "#   language: {\n",
    "#       \"dataset\": {\n",
    "#            split (dev/devtest): {\n",
    "#                \"raw\": raw dataset (without tokenization),\n",
    "#                \"tokenized\": tokenized dataset\n",
    "#            }\n",
    "#       }, \n",
    "#       \"dataloader\": None}\n",
    "#   }\n",
    "# }\n",
    "dataset_per_lang = {}\n",
    "for language in LANGUAGES:\n",
    "    print(f\"Loading dataset for {language}\", end=\"... \")\n",
    "\n",
    "    # add a dataloader key set to None, they are defined in the cell tagged\n",
    "    # @dataloader-creation\n",
    "    dataset_per_lang[language] = {\"dataset\": {}, \"dataloader\": None}\n",
    "\n",
    "    for split in USE_SPLITS:\n",
    "        dataset_per_lang[language][\"dataset\"][split] = {}\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"raw\"] = load_dataset(\n",
    "            DATA_SET_NAME,\n",
    "            language,\n",
    "            split=split,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=\"../cache/languages\",\n",
    "        )\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501481\n",
      "{'id': Value(dtype='int32', id=None), 'URL': Value(dtype='string', id=None), 'domain': Value(dtype='string', id=None), 'topic': Value(dtype='string', id=None), 'has_image': Value(dtype='int32', id=None), 'has_hyperlink': Value(dtype='int32', id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'dev': SplitInfo(name='dev', num_bytes=245488, num_examples=997, shard_lengths=None, dataset_name='flores'), 'devtest': SplitInfo(name='devtest', num_bytes=255993, num_examples=1012, shard_lengths=None, dataset_name='flores')}\n"
     ]
    }
   ],
   "source": [
    "# let's look at the English subset\n",
    "EX_DATASET_LANG = \"eng_Latn\"\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.dataset_size)\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.features)\n",
    "print(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][\"dev\"][\"raw\"].info.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing raw samples from eng_Latn:\n",
      "\n",
      "\tFirst sample from dev split:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1,\n",
      "\t\t 'sentence': 'On Monday, scientists from the Stanford University School of '\n",
      "\t\t             'Medicine announced the invention of a new diagnostic tool that '\n",
      "\t\t             'can sort cells by type: a tiny printable chip that can be '\n",
      "\t\t             'manufactured using standard inkjet printers for possibly about '\n",
      "\t\t             'one U.S. cent each.',\n",
      "\t\t 'topic': 'health'}\n",
      "\n",
      "\t996-th sample from dev split:\n",
      "\t\t{'URL': 'https://en.wikivoyage.org/wiki/Funeral_travel',\n",
      "\t\t 'domain': 'wikivoyage',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 997,\n",
      "\t\t 'sentence': 'In all cases, you must book by phone directly with the airline.',\n",
      "\t\t 'topic': 'Reason to travel/Funeral travel'}\n",
      "\n",
      "\tFirst sample from devtest split:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Toronto_team-led_research_on_Type_1_Diabetes_%27groundbreaking%27',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1,\n",
      "\t\t 'sentence': '\"We now have 4-month-old mice that are non-diabetic that used to '\n",
      "\t\t             'be diabetic,\" he added.',\n",
      "\t\t 'topic': 'disease, research, canada'}\n",
      "\n",
      "\t1011-th sample from devtest split:\n",
      "\t\t{'URL': 'https://en.wikivoyage.org/wiki/Working_and_studying_in_Japan',\n",
      "\t\t 'domain': 'wikivoyage',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 1012,\n",
      "\t\t 'sentence': \"Workers must often get their superiors' approval for any \"\n",
      "\t\t             \"decisions they make, and are expected to obey their superiors' \"\n",
      "\t\t             'instructions without question.',\n",
      "\t\t 'topic': 'Reason to travel/Working in Japan'}\n"
     ]
    }
   ],
   "source": [
    "# let's look at an individual sample from the dataset\n",
    "def get_sample(idx: int, lang: str, split: str, data: str):\n",
    "    return dataset_per_lang[lang]['dataset'][split][data][idx]\n",
    "\n",
    "print(f\"Viewing raw samples from {EX_DATASET_LANG}:\")\n",
    "for split in USE_SPLITS:\n",
    "    first_sample = get_sample(0, EX_DATASET_LANG, split, \"raw\")\n",
    "    last_sample = get_sample(-1, EX_DATASET_LANG, split, \"raw\")\n",
    "    dataset_len = len(dataset_per_lang[EX_DATASET_LANG][\"dataset\"][split][\"raw\"]) - 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"\\tFirst sample from {split} split:\")\n",
    "    pprint_tab(first_sample, indent=\"\\t\\t\")\n",
    "    print(\"\")\n",
    "    print(f\"\\t{dataset_len}-th sample from {split} split:\")\n",
    "    pprint_tab(last_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=\"../cache/tokenizers\")\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.unk_token})\n",
    "\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        raw_dataset = copy(dataset_per_lang[language][\"dataset\"][split][\"raw\"])\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"tokenized\"] = raw_dataset\n",
    "        dataset_per_lang[language][\"dataset\"][split][\"tokenized\"].set_transform(\n",
    "            tokenization\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viewing 17-th sample from eng_Latn:\n",
      "\tRaw sample:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Investigation_of_Deutsche_Bank_headquarters_spills_into_second_day',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 18,\n",
      "\t\t 'sentence': 'British newspaper The Guardian suggested Deutsche Bank '\n",
      "\t\t             'controlled roughly a third of the 1200 shell companies used to '\n",
      "\t\t             'accomplish this.',\n",
      "\t\t 'topic': 'crime'}\n",
      "\n",
      "\tTokenized sample:\n",
      "\t\ttensor([     2,  23409, 123980,    268,  67521, 102943,  22532,   5355, 170318,\n",
      "\t\t             6, 208717,     11,  27643,     48,     32,  27933, 105094,  33409,\n",
      "\t\t          3964,     33, 169662,    319,      5,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1])\n",
      "Viewing 17-th sample from spa_Latn:\n",
      "\tRaw sample:\n",
      "\t\t{'URL': 'https://en.wikinews.org/wiki/Investigation_of_Deutsche_Bank_headquarters_spills_into_second_day',\n",
      "\t\t 'domain': 'wikinews',\n",
      "\t\t 'has_hyperlink': 0,\n",
      "\t\t 'has_image': 0,\n",
      "\t\t 'id': 18,\n",
      "\t\t 'sentence': 'El periódico británico The Guardian señaló que Deutsche Bank '\n",
      "\t\t             'tenía el control de cerca de la tercera parte de las 1200 '\n",
      "\t\t             'compañías fantasma que se usaron para llevar esto a cabo.',\n",
      "\t\t 'topic': 'crime'}\n",
      "\n",
      "\tTokenized sample:\n",
      "\t\ttensor([     2,    453, 190847, 200580,     19,    268,  67521, 167160,     52,\n",
      "\t\t         22532,   5355,  39693,     85,   2867,     10,   9431,     10,     24,\n",
      "\t\t         82881,   1646,     10,    504,  27933, 218690, 190560,     52,     41,\n",
      "\t\t         18207,    231,    142,  31694,  11050,     11,  35787,      5,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "\t\t             1,      1])\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "LOOKAT_SAMPLE_ID = 17\n",
    "\n",
    "# get raw and tokenized sample\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")[\n",
    "    \"input_ids\"\n",
    "]\n",
    "\n",
    "print(f\"Viewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")\n",
    "\n",
    "EX_DATASET_LANG = \"spa_Latn\"\n",
    "\n",
    "raw_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"raw\")\n",
    "tokenized_sample = get_sample(LOOKAT_SAMPLE_ID, EX_DATASET_LANG, \"dev\", \"tokenized\")[\n",
    "    \"input_ids\"\n",
    "]\n",
    "\n",
    "print(f\"Viewing {LOOKAT_SAMPLE_ID}-th sample from {EX_DATASET_LANG}:\")\n",
    "print(\"\\tRaw sample:\")\n",
    "pprint_tab(raw_sample, indent=\"\\t\\t\")\n",
    "print(\"\\n\\tTokenized sample:\")\n",
    "pprint_tab(tokenized_sample, indent=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "dataloader-creation"
    ]
   },
   "outputs": [],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "BATCH_SIZE = 2  # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        tokenized_batch = dataset_per_lang[language][\"dataset\"][split][\"tokenized\"]\n",
    "        dataset_per_lang[language][\"dataloader\"] = torch.utils.data.DataLoader(\n",
    "            tokenized_batch, batch_size=BATCH_SIZE, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 1024, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained model from the huggingface hub\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, cache_dir=\"../cache/models\")\n",
    "\n",
    "# specify device on model and put the model into evaluation mode\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# TODO:\n",
    "# iterate over the dataset for each language and compute the cross-entropy loss per batch \n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        dataloader = dataset_per_lang[language][\"dataloader\"]\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"input_ids\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs, labels=labels)\n",
    "                loss = outputs.loss.item()\n",
    "            \n",
    "            losses[language].append(loss)\n",
    "for language in dataset_per_lang:\n",
    "    for split in dataset_per_lang[language][\"dataset\"]:\n",
    "        dataloader = dataset_per_lang[language][\"dataloader\"]\n",
    "        for batch in dataloader:\n",
    "            # TODO: compute cross-entropy loss per batch\n",
    "            # loss = ...\n",
    "            # losses[language].append(loss)\n",
    "            pass\n",
    "\n",
    "losses[\"eng_Latn\"].append(np.linspace(0, 1, 10))\n",
    "losses[\"eng_Latn\"].append(np.linspace(1, 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# create a bar plot for each language\n",
    "x = np.arange(len(LANGUAGES))\n",
    "y = [np.mean(losses[\"eng_Latn\"][1]) for language in LANGUAGES]\n",
    "\n",
    "axes.bar(x, y)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# format plot\n",
    "axes.set_xlabel(\"Language\") # x-axis label\n",
    "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
    "axes.set_xticklabels(losses.keys()) # x-axis tick labels\n",
    "axes.set_ylabel(\"Loss\") # y-axis label\n",
    "axes.set_ylim(0, 9) # range of y-axis\n",
    "axes.set_title(MODEL_NAME); # title\n",
    "axes.grid(True, which='major', color='k', linestyle='-', alpha=0.2)\n",
    "axes.grid(True, which='minor', color='k', linestyle='--', alpha=0.1)\n",
    "axes.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing XGLM to GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to re-run the analysis above, but using `gpt2` as the pre-trained language model. For this exercise, focus on your native language, unless it's English or isn't covered by flores. In that case, pick another language that you can read well. \n",
    "\n",
    "Compare the language modeling loss of XGLM and GPT2. What do you observe? Investigate the differences in tokenization for XGLM and GPT2. What do you observe? How can the good (or bad) performance of GPT2 be explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
