{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Visualize hidden representations of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, pardir, environ as os_environ\n",
    "from os.path import abspath, exists as path_exists\n",
    "from os.path import join as path_join\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from os import getcwd, pardir\n",
    "from os.path import abspath, isdir\n",
    "from os.path import join as path_join\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you're viewing this on Google Colab\n",
    "# download the requirements.txt hosted on a Github public gist and install the required libraries\n",
    "if 'COLAB_GPU' in os_environ:\n",
    "    !wget https://gist.githubusercontent.com/CamiloMartinezM/7eb6f86e2642e24776c90433c29133de/raw/368bcd1f53aecfb4b9ca9bf1090730d47872158a/requirements.txt -O requirements.txt\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    # For t-SNE to run on GPU, using tsnecuda installable on Colab with these commands\n",
    "    # See: https://inside-machinelearning.com/en/use-tsne-cuda-on-google-colab/\n",
    "    !pip install tsnecuda\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()\n",
    "    !wget https://anaconda.org/CannyLab/tsnecuda/3.0.0/download/linux-64/tsnecuda-3.0.0-h1234567_py3.8_73_cuda10.2.tar.bz2\n",
    "    !tar xvjf tsnecuda-3.0.0-h1234567_py3.8_73_cuda10.2.tar.bz2\n",
    "\n",
    "    if not path_exists(\"/usr/local/lib/python3.10/dist-packages/\"):\n",
    "        raise Exception(\"Please change the cp command param to be the actual directory to Colab's Python dist-packages\")\n",
    "    else:\n",
    "        !cp -r site-packages/* /usr/local/lib/python3.10/dist-packages/\n",
    "        !conda install --offline tsnecuda-3.0.0-h1234567_py3.8_73_cuda10.2.tar.bz2\n",
    "\n",
    "        !echo $LD_LIBRARY_PATH \n",
    "        ld_library_paths = os_environ['LD_LIBRARY_PATH'].split(':')\n",
    "        libfaiss_path = '/content/lib/libfaiss.so'\n",
    "\n",
    "        for path in ld_library_paths:\n",
    "            symlink_path = path_join(path, 'libfaiss.so')\n",
    "            # Check if the path exists before trying to create a symlink\n",
    "            if isdir(path) and not path_exists(symlink_path):\n",
    "                !ln -s {libfaiss_path} {symlink_path}\n",
    "                print(f\"Created symlink in {path}\")\n",
    "            else:\n",
    "                print(f\"Path {path} does not exist or symlink already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = abspath(path_join(getcwd(), pardir))\n",
    "\n",
    "# Add to path the parent directory titled NNTIProject/\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# 'scripts' directory is one level down from the parent directory\n",
    "# Add it to sys.path if it's not already there\n",
    "scripts_dir = path_join(parent_dir, \"scripts\")\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.append(scripts_dir)\n",
    "\n",
    "# Import task2.py\n",
    "from task2 import Task2Runner, LANGUAGES, Task2Plotter, RUNNER_CACHE_DIR\n",
    "\n",
    "# Use Google Colab's content directory if running on Google Colab, otherwise use local cache\n",
    "CACHE_DIR = \"/content/cache/\" if 'COLAB_GPU' in os_environ else RUNNER_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-03-16 16:49:50.261801\n",
      "\n",
      "Loading dataset for eng_Latn (devtest)... Skipped\n",
      "Loading dataset for spa_Latn (devtest)... Skipped\n",
      "Loading dataset for deu_Latn (devtest)... Skipped\n",
      "Loading dataset for arb_Arab (devtest)... Skipped\n",
      "Loading dataset for tam_Taml (devtest)... Skipped\n",
      "Loading dataset for quy_Latn (devtest)... Skipped\n",
      "\n",
      "Running t-SNE for layer 0 of facebook/xglm-564M...\n",
      "\n",
      "Initial setup of Token Representations:\n",
      "\tShape of array for ('eng_Latn', 'devtest'): (6009, 1024)\n",
      "\tShape of array for ('spa_Latn', 'devtest'): (7424, 1024)\n",
      "\tShape of array for ('deu_Latn', 'devtest'): (7412, 1024)\n",
      "\tShape of array for ('arb_Arab', 'devtest'): (7707, 1024)\n",
      "\tShape of array for ('tam_Taml', 'devtest'): (9271, 1024)\n",
      "\tShape of array for ('quy_Latn', 'devtest'): (8591, 1024)\n",
      "Initial setup of Sentence Representations:\n",
      "\tShape of array for ('eng_Latn', 'devtest'): (200, 1024)\n",
      "\tShape of array for ('spa_Latn', 'devtest'): (200, 1024)\n",
      "\tShape of array for ('deu_Latn', 'devtest'): (200, 1024)\n",
      "\tShape of array for ('arb_Arab', 'devtest'): (200, 1024)\n",
      "\tShape of array for ('tam_Taml', 'devtest'): (200, 1024)\n",
      "\tShape of array for ('quy_Latn', 'devtest'): (200, 1024)\n",
      "Final setup of ordered token representations: (46414, 1024)\n",
      "Final setup of ordered sentence representations: (1200, 1024)\n",
      "On Token representations...\n",
      "\tApplying t-SNE (perplexity=464, learning_rate=3867)... "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print(f\"Start time: {datetime.now()}\\n\")\n",
    "for dim_reduction in [\"t-SNE\"]: # [\"PCA\", \"t-SNE\"]:\n",
    "    for model_name in [\"facebook/xglm-564M\"]:\n",
    "        # Create a Task2Runner instance for the current model and run it\n",
    "        runner = Task2Runner(\n",
    "            LANGUAGES,\n",
    "            [\"devtest\"],  # Only analyzing the devtest split\n",
    "            model_name,\n",
    "            seq_by_seq=True,\n",
    "            subset=200,\n",
    "            cache_dir=CACHE_DIR,\n",
    "            perform_early_setup=False,\n",
    "        )\n",
    "        runner.run()\n",
    "\n",
    "        # Create a Task2Plotter instance for the current model, run the current dimensionality reduction technique\n",
    "        # and save each plot to disk for each layer\n",
    "        for layer in range(0, runner.num_layers + 1):\n",
    "            print(f\"\\nRunning {dim_reduction} for layer {layer} of {model_name}...\\n\")\n",
    "            plotter = Task2Plotter(\n",
    "                runner, layer=layer, \n",
    "                cache_dir=CACHE_DIR,\n",
    "                plots_folder=\"plots_task2_test\", \n",
    "                use_tsnecuda=True\n",
    "            )\n",
    "            plotter.run(dim_reduction=dim_reduction, check_plot_exists=True)\n",
    "\n",
    "            for ext in [\"png\", \"svg\"]:\n",
    "                plotter.plot(\n",
    "                    token_repr=True,\n",
    "                    sentence_repr=True,\n",
    "                    cmap=\"Accent\",\n",
    "                    save_to_disk=True,\n",
    "                    ext=ext,\n",
    "                    subfolder_for_ext=True,\n",
    "                    edgecolor=\"black\",\n",
    "                    linewidth=0.1,\n",
    "                )\n",
    "\n",
    "            del plotter\n",
    "        del runner\n",
    "\n",
    "print(f\"\\nTotal elapsed time: {time() - start_time} s\")\n",
    "print(f\"End time: {datetime.now()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
