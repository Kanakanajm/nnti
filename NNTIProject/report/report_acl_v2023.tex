% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended. \pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\newcommand{\customsection}[1]{
  \noindent\textbf{#1}.\vspace{2mm}
}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Instructions for ACL 2023 Proceedings}

\author{Martínez, Camilo \\
  Universität des Saarlandes \\
  \texttt{cama00005@uni-saarland.de} \\\And
  Ma, Honglu \\
  Universität des Saarlandes \\
  \texttt{homa00001@uni-saarland.de} \\}

\begin{document}
\maketitle
\begin{abstract}
    This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style file for ACL 2023.
    The document itself conforms to its own specifications, and is, therefore, an example of what your manuscript should look like.
    These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}

These instructions.

\section{Background}

hola

\section{Experiments and Results}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

Hola

\subsection{Dimensionality Reduction Techniques}

On the other hand, for the t-SNE visualizations, we used the parameters on \cite{GOVE202287}.

\subsection{Fine-tuning the XGLM-564M Model}

\customsection{Full-finetuning} We implemented a full-finetuning approach, where the entire model's weights are relearned on the target task from scratch.

\customsection{LoRA} We implemented a Low-Rank Adaptation of Large Language Models or LoRA, introduced in the paper \cite{hu2021lora}.

\customsection{BitFit} We implemented Simple Parameter efficient Fine-tuning
for Transformer-based Masked Language-models, BitFit \cite{zaken2022bitfit}.

\customsection{(IA)\textsuperscript{3}} We implemented a PEFT method called (IA)\textsuperscript{3} that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters, as introduced in the paper \cite{liu2022fewshot}.

\section{Conclusions}

JADFSJKJDA

\section*{Limitations}
Due to the extensive computational requirements, the study was limited to a batch size of 2. This constraint could potentially affect the generalization of the findings. For instance, larger batch sizes may yield different embeddings and therefore, the representations of the hidden-layers of the model and potentially alter the provided PCA and t-SNE visualizations. Additionally, the analyses were focused on a selection of six languages: English, Spanish, German, Arabic, Tamil, and Quechua. Therefore, the results presented may not be universally applicable, particularly to languages with much different structures.

Furthermore, for the sensible feasibility of t-SNE computations on our datasets, we employed the fast implementation provided via the openTSNE library \cite{opentsne}, given our computational resources constraints. Although our study utilized the CS cluster, the requirement for substantial GPU resources limits the scalability of our methods, particularly for longer texts or larger datasets. This presents an avenue for further research to develop more resource-efficient algorithms capable of handling more extensive and linguistically diverse data without compromising on the fidelity of the representations.

\section*{Ethics Statement}
Our review work strictly adheres to the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We acknowledge that while our evaluation contributes to the understanding of language model behaviors and their applications, it does not introduce novel methodologies or data. We have carefully considered the ethical implications of our work, ensuring that our analyses do not misrepresent capabilities, and we recognize the importance of responsible AI development and the potential impact of language technologies on society. Our work aims to provide a comprehensive, unbiased evaluation to foster informed discussions within the research community. This review has been conducted with an emphasis on transparency and reproducibility, encouraging the ethical advancement of natural language processing research.

\section*{Acknowledgements}
This work was conducted as a final project for the course "Neural Networks: Theory and Implementation," instructed by Prof. Dietrich Klakow at Universität des Saarlandes. We extend our gratitude to the course's Team for their guidance on this project and to the University. This document format and styling has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).