\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{3}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: Camilo MartÃ­nez 7057573, Honglu Ma 7055053}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Linear Regression}

    \subsection*{(a)}
Plug the two points in the equation, we get:
\begin{align*} 
2w + b &= 4 \\ 
4w + b &=  5
\end{align*}
Solve the equations, we have:
\begin{align*} 
w &= \frac{1}{2}\\
b &= 3
\end{align*}
    \subsection*{(b)}
The point (3, 6) does not lie on the line above. \\
\textbf{Proof by contradiction}: Assume that the point lies on the line, it must satisfy the equation $Y = \frac{1}{2}X+3$. Plug the point in the equation, which yields: $\frac{9}{2} = 6$, which is false.
    \subsection*{(c, matrix form)}
    By using the result we get from Exercise 2d in Assignment 2 $$f(x) = ||Bx - c||_2^2, \nabla_xf(x) = 2B^\top(Bx - c)$$ we get:
    $$\nabla_w\mathrm{MSE}_{train} = \nabla_w\frac{1}{m}||X_{train}w - y_{train}||_2^2 = \frac{1}{m}(2X_{train}^\top(X_{train}w - y_{train})) = X_{train}^\top X_{train}w - X_{train}^\top y_{train} = 0$$\\ which can be written as\\
    $$X_{train}^\top X_{train}w = X_{train}^\top y_{train}$$\\
    so\\
    $$w = (X_{train}^\top X_{train})^{-1} X_{train}^\top y_{train}$$\\
    
    \subsection*{(c)}
    The Mean Squared Error (MSE) equation is given by:
    
    \begin{equation}\label{first}
        \mathrm{MSE}_{train} = \frac{1}{m}\sum_{i=1}^m {(\hat{y}_{train}^{(i)} - y_{train}^{(i)})^2}
    \end{equation}
    
    for a linear regression model \(Y = wX + b\), where \(\hat{y}_{train}^{(i)}\) is the predicted value, \(y_{train}^{(i)}\) is the actual value, and \(m\) is the number of training examples.\\

    From the linear regression model equation, we know that \(\hat{y}_{train}^{(i)} = wX^{(i)} + b\). We can plug this into (\ref{first}) and get:
    
    \begin{equation}\label{second}
        \mathrm{MSE}_{train} = \frac{1}{m}\sum_{i=1}^m {(wX^{(i)} + b - y_{train}^{(i)})^2}
    \end{equation}

    To minimize (\ref{second}) and find the optimal weight $w$, we first need to find the optimal value of $b$. To do this, we calculate the derivative of MSE with respect to \(b\) and set it to zero:
    \begin{alignat*}{2}
        \frac{\partial \mathrm{MSE}_{train}}{\partial b} &= 0\\
        \Rightarrow\quad \frac{2}{m} \sum_{i=1}^m {(wX^{(i)} + b - y_{train}^{(i)})} &= 0\\
        \Rightarrow\quad \sum_{i=1}^m {w(X^{(i)})} + \sum_{i=1}^m {b} &= \sum_{i=1}^m {y_{train}^{(i)}}\\
        \Rightarrow\quad m b & = \sum_{i=1}^m {y_{train}^{(i)}} - \sum_{i=1}^m {w(X^{(i)})}\\
        \Rightarrow\quad b & = \frac{1}{m}\sum_{i=1}^m {y_{train}^{(i)}} - w \frac{1}{m} \sum_{i=1}^m {X^{(i)}}
    \end{alignat*}

    If we define \(\bar{y} = \frac{1}{m}\sum_{i=1}^m {y_{train}^{(i)}}\) and \(\bar{X} = \frac{1}{m} \sum_{i=1}^m {X^{(i)}}\), which respectively are the average of all training outputs \(y_{train}\) and average of all training inputs $X$, we can express the previous equation as:

    \begin{equation}\label{third}
        b = \bar{y} - w \bar{X}    
    \end{equation}
    
    Now, we can plug (\ref{third}) into (\ref{second}), calculate the derivative with respect to $w$ and set it to zero to calculate the optimal value of $w$.

    \[
        \begin{split}
            \frac{\partial}{\partial w}
            \begin{bmatrix}
                \frac{1}{m} \sum_{i=1}^m {(wX^{(i)} + \bar{y} - w \bar{X} - y_{train}^{(i)})^2}
            \end{bmatrix}
            &= 0\\
            \Rightarrow\quad \frac{\partial}{\partial w}
            \begin{bmatrix}
                \frac{1}{m} \sum_{i=1}^m {(w(X^{(i)} - \bar{X}) - (y_{train}^{(i)} - \bar{y}))^2}
            \end{bmatrix} 
            &= 0\\
            \Rightarrow\quad \frac{2}{m} \sum_{i=1}^m {(w(X^{(i)} - \bar{X}) - (y_{train}^{(i)} - \bar{y}))(X^{(i)} - \bar{X})}
            & = 0\\
            \Rightarrow\quad w \sum_{i=1}^m {(X^{(i)} - \bar{X})^2} 
            &= \sum_{i=1}^m {(y_{train}^{(i)} - \bar{y})(X^{(i)} - \bar{X})}
        \end{split}
    \]
    
    Finally, we get the expression for \(w\) that minimizes the MSE:
    
    \[
        w = \frac{\sum_{i=1}^m {(y_{train}^{(i)} - \bar{y})(X^{(i)} - \bar{X})}}{ \sum_{i=1}^m {(X^{(i)} - \bar{X})^2}}
    \]
    
    \subsection*{(d)}
    The difference between the line and x value can be seen as $f^{-1}(y_{train}) - X_{train}$ where we use the intercept $y$ to get the feature $X$ which is not what we want to achieve.
\end{homeworkProblem}

\begin{homeworkProblem}{- PCA as Autoencoder}
\subsection*{(a)}
We minimize the reconstruction error by solving $\nabla_c||x - Dc||_2^2  = 0$:
\begin{alignat*}{2}
                    & & \nabla_c\,||x - Dc||_2^2  & = 0\\
   \Rightarrow\quad & & \nabla_c\,(x - Dc)^\top (x - Dc) & = 0\\
   \Rightarrow\quad & & \nabla_c\,(x^\top - (Dc)^\top)(x - Dc)& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - (Dc)^\top x + (Dc)^\top (Dc)& = 0\tag{1}\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - ((Dc)^\top x)^\top + (Dc)^\top (Dc)& = 0\tag{2}\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - x^\top Dc + (Dc)^\top (Dc)& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top(D^\top D)c& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top I_l c& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top c& = 0\tag{3}\\
   \Rightarrow\quad & & 0 - 2 D^\top x + 2c & = 0\tag{4}\\
   \Rightarrow\quad & & c & = D^\top x
\end{alignat*}
\textbf{Clarification of Step 0 to (1)}: because $(Dc)^\top x$ is a scalar so $(Dc)^\top x = ((Dc)^\top x)^\top$.\\
\textbf{Clarification of Step (2) to (3)}: we calculate the gradient with respect to $c$; the first term $\nabla_c\,x^\top x = 0$ because it does not have $c$ term, it acts as a constant; the second term can be rewritten as $\nabla_c\, -2(D^\top x)^\top c$ so that we can apply the result of Exercise 2a in Assignment 2, we get $\nabla_c\, -2(D^\top x)^\top c = D^\top x$; the third term can be rewritten as $\nabla_c c^\top I_l c$ and by the result of Exercise 2b in Assignment 2 we know that $\nabla_c c^\top I_l c = I_l c + I_l^\top c = 2c$

\newcommand{\argmind}{\underset{D}{\rm argmin}\,}
\newcommand{\argmaxd}{\underset{D}{\rm argmax}\,}

\newcommand{\argmaxsd}{\underset{d}{\rm argmax}\,}
\subsection*{(b)}
Firstly we show that $\argmind||X-XDD^\top||_F^2 = \argmaxd\sum_{i=1}^{m}D_{.i}^\top X^\top XD_{.i}$
\begin{align*}
\argmind||X-XDD^\top||_F^2& = \argmind Tr((X-XDD^\top)^\top(X-XDD^\top))\\
&= \argmind Tr((X^\top - DD^\top X^\top)(X - XDD^\top))\\
&= \argmind Tr(X^\top X - X^\top XDD^\top - DD^\top X^\top X + DD^\top X^\top XDD^\top)\\
&= \argmind -Tr(X^\top XDD^\top) - Tr(DD^\top X^\top X) + Tr(DD^\top X^\top XDD^\top)\\
&= \argmind-Tr(X^\top XDD^\top) - Tr((DD^\top X^\top X)^\top) + Tr(DD^\top X^\top XDD^\top)\\
&= \argmind-Tr(X^\top XDD^\top) - Tr(X^\top XDD^\top) + Tr(DD^\top X^\top XDD^\top)\\
&= \argmind-2Tr(X^\top XDD^\top) + Tr(DD^\top X^\top XDD^\top)\\
\intertext{since $DD^\top X^\top X \in\mathbb{R}^{m\times m}, DD^\top \in\mathbb{R}^{m\times m}$}\\
&= \argmind-2Tr(X^\top XDD^\top) + Tr(DD^\top DD^\top X^\top X)\\
\intertext{since $DD^\top DD^\top \in\mathbb{R}^{m\times m},X^\top X \in\mathbb{R}^{m\times m}$}\\
&= \argmind-2Tr(X^\top XDD^\top) + Tr(X^\top X DD^\top DD^\top)\\
&= \argmind-2Tr(X^\top XDD^\top) + Tr(X^\top X DI_lD^\top)\\
&= \argmind-2Tr(X^\top XDD^\top) + Tr(X^\top X DD^\top)\\
&= \argmind-Tr(X^\top XDD^\top)\\
&= \argmaxd Tr(X^\top XDD^\top)\\
\intertext{since $X^\top X D \in \mathbb{R}^{m \times l}, D^\top \in \mathbb{R}^{l \times m}$}\\
&= \argmaxd Tr(D^\top X^\top XD)\\
&= \argmaxd ||XD||_F^2\\
\intertext{we know that $XD = (D^\top X^\top)^\top$ and $D^\top X^\top$ gives a matrix where the every column is a compressed $x^{(i)}$, i.e. $c^{(i)}$ and the Frobenius norm is just summing the square of all elements of $c^{(i)}$ for all i. With $\sum_{i = 1}^m D^\top_{.i} X^\top$ we get the sum of all elements of all $c$ and with $\sum_{i = 1}^m D^\top_{.i} X^\top X D_{.i}$ we get the sum of the square of all elements of all $c$}\\
&= \argmaxd \sum_{i = 1}^m D^\top_{.i} X^\top X D_{.i}\\
\end{align*}
Now we can instead show the matrix $D$ whose columns are the eigenvectors of $X^\top X$ solves $\argmaxd \sum_{i = 1}^m D^\top_{.i} X^\top X D_{.i}$ for $D D^T D = I_l, l > 0$ by induction on $l$.\\\\
\textbf{Base Step} $d^\top d = 1$\\
Since d is now a vector, we get $\argmaxsd \sum_{i = 1}^m d^\top_{.i} X^\top X d_{.i} =\argmaxsd d^\top X^\top X d$, i.e. we want to find a $d\star$ such that $f(d\star)$ is the maximum of $f(d) = d^\top X^\top X d$ and with constraint $g(d\star) = 1$ where $g(d) = d^\top d$. We can achieve this by solving $\nabla_d f = k \nabla_d g$
\newcommand{\gradd}{\nabla_d\,}
\begin{align*}
\gradd f(d) = k \gradd g(d) &\Rightarrow \gradd d^\top X^\top Xd = k \gradd d^\top d\\
&\Rightarrow X^\top Xd + (X^\top X)^\top d = 2kd\\
&\Rightarrow 2X^\top Xd = 2kd\\
&\Rightarrow X^\top Xd = kd\\
\intertext{so that $k$ is a eigenvalue of $X^\top X$ and $d$ is the eigenvector. We have $d^\top X^\top Xd = d^\top kd = k d^\top d = k$ thus $k$ must be the largest eigenvalue and $d$ be the corresponding eigenvector}.
\end{align*}
\textbf{Induction Step}\\
Now assume that for $D_n^\top D_n = I_n, \sum_{i = 1}^m D^\top_{n.i} X^\top X D_{n.i}$ has maximum when $D_{n.i}$ are the n largest eigenvectors of $X^\top X$. We want to show that it is also the case for $D_{n+1}^\top D_{n+1} = I_{n+1}$ where the first n columns of $D_{n+1}$ are the same as the ones in $D_{n}$ and the last column is the $(n+1)^{th}$ largest eigenvector of $X^\top X$.
\begin{align*}
\sum_{i = 1}^m D^\top_{n+1.i} X^\top X D_{n+1.i} &= \sum_{i = 1}^m D^\top_{n.i} X^\top X D_{n.i} + D^\top_{n+1.n+1} X^\top X D_{n+1.n+1}\\
\intertext{notice that the term $\sum_{i = 1}^m D^\top_{n.i} X^\top X D_{n.i}$ is already the largest by the induction hypothesis, now the question is reduced to finding $\rm argmax\,D^\top_{n+1.n+1} X^\top X D_{n+1.n+1} $. Since $D_{n+1.n+1}$ is a vector we can repeat the steps of our base case to get that $D_{n+1.n+1}$ is a eigenvector of $X^\top X$. To not violate the orthogonality constraint we know that $D_{n+1.n+1}$ must be the corresponding eigenvector of the $(n+1)^{th}$ largest eigenvalue.}\\
\end{align*}
 

\end{homeworkProblem}

\begin{homeworkProblem}{- PCA}
    See attached .ipynb solution in .zip file.

\end{homeworkProblem}

\end{document}