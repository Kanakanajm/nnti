\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{3}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: Camilo Mart√≠nez 7057573, Honglu Ma 7055053}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Linear Regression}

    \subsection*{(a)}
Plug the two points in the equation, we get:
\begin{align*} 
2w + b &= 4 \\ 
4w + b &=  5
\end{align*}
Solve the equations, we have:
\begin{align*} 
w &= \frac{1}{2}\\
b &= 3
\end{align*}
    \subsection*{(b)}
It does not lie on the line above. \\
Proof by contradiction:\\
Assume that the point lies on the line, it must satisfy the equation $Y = \frac{1}{2}X+3$.\\
Plug the point in the equation, we get: $\frac{9}{2} = 6$ which is false.
    \subsection*{(c, matrix form)}
    By using the result we get from Exercise 2d in Assignment 2 $$f(x) = ||Bx - c||_2^2, \nabla_xf(x) = 2B^\top(Bx - c)$$ we get:
    $$\nabla_w\mathrm{MSE}_{train} = \nabla_w\frac{1}{m}||X_{train}w - y_{train}||_2^2 = \frac{1}{m}(2X_{train}^\top(X_{train}w - y_{train})) = X_{train}^\top X_{train}w - X_{train}^\top y_{train} = 0$$\\ which can be written as\\
    $$X_{train}^\top X_{train}w = X_{train}^\top y_{train}$$\\
    so\\
    $$w = (X_{train}^\top X_{train})^{-1} X_{train}^\top y_{train}$$\\
    
    \subsection*{(c)}
    The Mean Squared Error (MSE) equation is given by:
    
    \begin{equation}\label{first}
        \mathrm{MSE}_{train} = \frac{1}{m}\sum_{i=1}^m {(\hat{y}_{train}^{(i)} - y_{train}^{(i)})^2}
    \end{equation}
    
    for a linear regression model \(Y = wX + b\), where \(\hat{y}_{train}^{(i)}\) is the predicted value, \(y_{train}^{(i)}\) is the actual value, and \(m\) is the number of training examples.\\

    From the linear regression model equation, we know that \(\hat{y}_{train}^{(i)} = wX^{(i)} + b\). We can plug this into (\ref{first}) and get:
    
    \begin{equation}\label{second}
        \mathrm{MSE}_{train} = \frac{1}{m}\sum_{i=1}^m {(wX^{(i)} + b - y_{train}^{(i)})^2}
    \end{equation}

    To minimize (\ref{second}) and find the optimal weight $w$, we first need to find the optimal value of $b$. To do this, we calculate the derivative of MSE with respect to \(b\) and set it to zero:
    \begin{alignat*}{2}
        \frac{\partial \mathrm{MSE}_{train}}{\partial b} &= 0\\
        \Rightarrow\quad \frac{2}{m} \sum_{i=1}^m {(wX^{(i)} + b - y_{train}^{(i)})} &= 0\\
        \Rightarrow\quad \sum_{i=1}^m {w(X^{(i)})} + \sum_{i=1}^m {b} &= \sum_{i=1}^m {y_{train}^{(i)}}\\
        \Rightarrow\quad m b & = \sum_{i=1}^m {y_{train}^{(i)}} - \sum_{i=1}^m {w(X^{(i)})}\\
        \Rightarrow\quad b & = \frac{1}{m}\sum_{i=1}^m {y_{train}^{(i)}} - w \frac{1}{m} \sum_{i=1}^m {X^{(i)}}
    \end{alignat*}

    If we define \(\bar{y} = \frac{1}{m}\sum_{i=1}^m {y_{train}^{(i)}}\) and \(\bar{X} = \frac{1}{m} \sum_{i=1}^m {X^{(i)}}\), which respectively are the average of all training outputs \(y_{train}\) and average of all training inputs $X$, we can express the previous equation as:

    \begin{equation}\label{third}
        b = \bar{y} - w \bar{X}    
    \end{equation}
    
    Now, we can plug (\ref{third}) into (\ref{second}), calculate the derivative with respect to $w$ and set it to zero to calculate the optimal value of $w$.

    \[
        \begin{split}
            \frac{\partial}{\partial w}
            \begin{bmatrix}
                \frac{1}{m} \sum_{i=1}^m {(wX^{(i)} + \bar{y} - w \bar{X} - y_{train}^{(i)})^2}
            \end{bmatrix}
            &= 0\\
            \Rightarrow\quad \frac{\partial}{\partial w}
            \begin{bmatrix}
                \frac{1}{m} \sum_{i=1}^m {(w(X^{(i)} - \bar{X}) - (y_{train}^{(i)} - \bar{y}))^2}
            \end{bmatrix} 
            &= 0\\
            \Rightarrow\quad \frac{2}{m} \sum_{i=1}^m {(w(X^{(i)} - \bar{X}) - (y_{train}^{(i)} - \bar{y}))(X^{(i)} - \bar{X})}
            & = 0\\
            \Rightarrow\quad w \sum_{i=1}^m {(X^{(i)} - \bar{X})^2} 
            &= \sum_{i=1}^m {(y_{train}^{(i)} - \bar{y})(X^{(i)} - \bar{X})}
        \end{split}
    \]
    
    Finally, we get the expression for \(w\) that minimizes the MSE:
    
    \[
        w = \frac{\sum_{i=1}^m {(y_{train}^{(i)} - \bar{y})(X^{(i)} - \bar{X})}}{ \sum_{i=1}^m {(X^{(i)} - \bar{X})^2}}
    \]
    
    \subsection*{(d)}
    The difference between the line and x value can be seen as $f^{-1}(y_{train}) - X_{train}$ where we use the intercept $y$ to get the feature $X$ which is not what we want to achieve.
\end{homeworkProblem}

\begin{homeworkProblem}{ - PCA as Autoencoder}
\subsection*{(a)}
We minimize the reconstruction error by solving $\nabla_c||x - Dc||_2^2  = 0$:
\begin{alignat*}{2}
                    & & \nabla_c\,||x - Dc||_2^2  & = 0\\
   \Rightarrow\quad & & \nabla_c\,(x - Dc)^\top (x - Dc) & = 0\\
   \Rightarrow\quad & & \nabla_c\,(x^\top - (Dc)^\top)(x - Dc)& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - (Dc)^\top x + (Dc)^\top (Dc)& = 0\tag{1}\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - ((Dc)^\top x)^\top + (Dc)^\top (Dc)& = 0\tag{2}\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - x^\top Dc - x^\top Dc + (Dc)^\top (Dc)& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top(D^\top D)c& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top I_l c& = 0\\
   \Rightarrow\quad & & \nabla_c\,x^\top x - 2x^\top Dc + c^\top c& = 0\tag{3}\\
   \Rightarrow\quad & & 0 - 2 D^\top x + 2c & = 0\tag{4}\\
   \Rightarrow\quad & & c & = D^\top x \\
\end{alignat*}
Clarification of Step 0 to 1: because $(Dc)^\top x$ is a scalar so $(Dc)^\top x = ((Dc)^\top x)^\top$; clarification of Step 2 to 3: we calculate the gradient with respect to $c$; the first term $\nabla_c\,x^\top x = 0$ because it does not have $c$ term, it acts as a constant; the second term can be rewritten as $\nabla_c\, -2(D^\top x)^\top c$ so that we can apply the result of Exercise 2a in Assignment 2, we get $\nabla_c\, -2(D^\top x)^\top c = D^\top x$; the third term can be rewritten as $\nabla_c c^\top I_l c$ and by the result of Exercise 2b in Assignment 2 we know that $\nabla_c c^\top I_l c = I_l c + I_l^\top c = 2c$

\subsection*{(b)}

\end{homeworkProblem}

\begin{homeworkProblem}{- PCA}
    See attached .ipynb solution in .zip file.

\end{homeworkProblem}

\end{document}