{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 1**\n",
        "\n",
        "This exercise will help you get familiar with NumPy library in python. The goal of this exercise\n",
        "is for you to explore the documentation of different NumPy libraries and their implementation\n",
        "details."
      ],
      "metadata": {
        "id": "lBS8e_UAHV23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHgrYGX1GdsI",
        "outputId": "89bc5a18-70f7-4541-82a0-6a7881953590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X:  (4, 16)\n",
            "X = \n",
            "\n",
            " [[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01\n",
            "  1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01\n",
            "  3.96767474e-01 5.38816734e-01 4.19194514e-01 6.85219500e-01\n",
            "  2.04452250e-01 8.78117436e-01 2.73875932e-02 6.70467510e-01]\n",
            " [4.17304802e-01 5.58689828e-01 1.40386939e-01 1.98101489e-01\n",
            "  8.00744569e-01 9.68261576e-01 3.13424178e-01 6.92322616e-01\n",
            "  8.76389152e-01 8.94606664e-01 8.50442114e-02 3.90547832e-02\n",
            "  1.69830420e-01 8.78142503e-01 9.83468338e-02 4.21107625e-01]\n",
            " [9.57889530e-01 5.33165285e-01 6.91877114e-01 3.15515631e-01\n",
            "  6.86500928e-01 8.34625672e-01 1.82882773e-02 7.50144315e-01\n",
            "  9.88861089e-01 7.48165654e-01 2.80443992e-01 7.89279328e-01\n",
            "  1.03226007e-01 4.47893526e-01 9.08595503e-01 2.93614148e-01]\n",
            " [2.87775339e-01 1.30028572e-01 1.93669579e-02 6.78835533e-01\n",
            "  2.11628116e-01 2.65546659e-01 4.91573159e-01 5.33625451e-02\n",
            "  5.74117605e-01 1.46728575e-01 5.89305537e-01 6.99758360e-01\n",
            "  1.02334429e-01 4.14055988e-01 6.94400158e-01 4.14179270e-01]]\n",
            "\n",
            "Shape of F:  (4, 2)\n",
            "F = \n",
            "\n",
            " [[414.9056398  390.07032484]\n",
            " [501.85928358 471.1409601 ]\n",
            " [610.49740621 574.00134214]\n",
            " [389.25848991 366.28424199]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1) # Makes results reproducible\n",
        "\n",
        "# Write a Feed Forward pass of a simple Neural Network:\n",
        "# ‚Äì Randomly generate a 4x16 input matrix X\n",
        "X = np.random.random((4, 16))\n",
        "print(\"Shape of X: \", X.shape)\n",
        "print(\"X = \\n\\n\", X)\n",
        "\n",
        "# ‚Äì Randomly generate three weight matrices W1, W2, W3 of sizes: 16x16, 32x16 2x32\n",
        "W1 = np.random.random((16, 16))\n",
        "W2 = np.random.random((32, 16))\n",
        "W3 = np.random.random((2, 32))\n",
        "\n",
        "# ‚Äì Perform the forward pass: (((ùëã ‚ãÖ ùëä1ùëá ) ‚ãÖ ùëä2ùëá ) ‚ãÖ ùëä3ùëá )\n",
        "F = np.matmul(np.matmul(np.matmul(X, W1.T), W2.T), W3.T)\n",
        "print(\"\\nShape of F: \", F.shape)\n",
        "print(\"F = \\n\\n\", F)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # Makes results reproducible\n",
        "\n",
        "# (Bonus) Generate a random 4x4 matrix and use NumPy library to compute eigenvalues and eigenvectors of the randomly\n",
        "# generated matrix.\n",
        "A = np.random.random((4, 4))\n",
        "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
        "\n",
        "print(\"\\n(Bonus)\\nEigenvalues (each value is an eigenvalue): \", eigenvalues)\n",
        "print(\"\\nEigenvectors (each row is an eigenvector):\\n\\n\", eigenvectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESiSi1EZHq85",
        "outputId": "493c2a8d-2f92-4e1f-8e78-1dcbc1180090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(Bonus)\n",
            "Eigenvalues (each value is an eigenvalue):  [ 1.35666782  0.25641859  0.19491643 -0.2089802 ]\n",
            "\n",
            "Eigenvectors (each row is an eigenvector):\n",
            "\n",
            " [[-0.38804596 -0.43214848  0.18691558  0.46649225]\n",
            " [-0.28996723 -0.12110532 -0.20205441 -0.62691493]\n",
            " [-0.70725243 -0.72791773 -0.89755477 -0.33272554]\n",
            " [-0.51491101  0.51837918  0.34442998  0.52787913]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise 2**\n",
        "\n",
        "This exercise will help you get familiar with PyTorch Dataset and Dataloader classes. The goal\n",
        "of the exercise is to implement your own Dataset and Dataloader classes for different types of data: tabular and sequential.\n",
        "\n",
        "### **Part 1**"
      ],
      "metadata": {
        "id": "pDTs3FNULqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# According to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import tensor, stack\n",
        "\n",
        "class CustomPhrasesDataset(Dataset):\n",
        "    def __init__(self, data_json_filepath, train=True, train_size=0.8, shuffle_seed=0, transform=None, target_transform=None):\n",
        "        with open(data_json_filepath) as f:\n",
        "            self.token_data = json.load(f)\n",
        "        self.shuffle(shuffle_seed)\n",
        "\n",
        "        # Checks that it's between 0 and 1, representing the % of the entire data that is training data\n",
        "        assert 0 <= train_size <= 1\n",
        "\n",
        "        # Partitions the data accordingly\n",
        "        if train:\n",
        "            self.token_data = self.token_data[:int(len(self.token_data) * train_size)]\n",
        "        else:\n",
        "            self.token_data = self.token_data[int(len(self.token_data) * train_size):]\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = tensor(self.token_data[idx]['tokens'])\n",
        "        label = tensor(self.token_data[idx]['label'])\n",
        "        if self.transform:\n",
        "            tokens = self.transform(tokens)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return tokens, label\n",
        "\n",
        "    def shuffle(self, seed):\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(self.token_data)\n",
        "\n",
        "# The tokens are in different sizes, we get the max and pad the missing\n",
        "def pad(input_tensor, desired_size):\n",
        "    return F.pad(input_tensor, (0, desired_size - input_tensor.size(0)), value=0)\n",
        "\n",
        "class CustomPhrasesDataLoader(DataLoader):\n",
        "    def __init__(self, data, batch_size, shuffle = False, shuffle_seed = 0):\n",
        "        # calculate the desired size for padding\n",
        "        len_tokens = max(len(ts) for ts, ls in data)\n",
        "\n",
        "        if (shuffle):\n",
        "            data.shuffle(shuffle_seed)\n",
        "\n",
        "        self.batches = [(\n",
        "            # data\n",
        "            stack([pad(tensor(data[j][0]), len_tokens) for j in range(i, min(i + batch_size, len(data)))]),\n",
        "            # labels\n",
        "            stack([tensor(data[j][1]) for j in range(i, min(i + batch_size, len(data)))])\n",
        "            )\n",
        "        for i in range(0, len(data), batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, itx):\n",
        "        return self.batches[itx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.batches:\n",
        "            yield batch\n",
        "\n",
        "# TEST\n",
        "# data2.csv should be in the current working directory\n",
        "data_json_filepath = os.path.join(os.getcwd(), 'data1.json')\n",
        "\n",
        "training_data = CustomPhrasesDataset(\n",
        "    data_json_filepath,\n",
        "    train=True\n",
        ")\n",
        "\n",
        "test_data = CustomPhrasesDataset(\n",
        "    data_json_filepath,\n",
        "    train=False\n",
        ")\n",
        "\n",
        "train_dataloader = CustomPhrasesDataLoader(training_data, batch_size=64, shuffle=False)\n",
        "test_dataloader = CustomPhrasesDataLoader(test_data, batch_size=64, shuffle=False)\n",
        "f, l = next(iter(train_dataloader))\n",
        "print(f\"\\nFeature batch shape: {f.size()}\\n\")\n",
        "print(f\"Labels batch shape: {l.size()}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXlTEStjL0ES",
        "outputId": "89c52dc6-1283-4117-c190-131fb69eef93"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-f793217ad810>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  stack([pad(tensor(data[j][0]), len_tokens) for j in range(i, min(i + batch_size, len(data)))]),\n",
            "<ipython-input-69-f793217ad810>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  stack([tensor(data[j][1]) for j in range(i, min(i + batch_size, len(data)))])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature batch shape: torch.Size([64, 238])\n",
            "\n",
            "Labels batch shape: torch.Size([64])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2**"
      ],
      "metadata": {
        "id": "RnevhRH4L-3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# According to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import tensor, stack\n",
        "\n",
        "class CustomPrognosisDataset(Dataset):\n",
        "    def __init__(self, data_csv_filepath, train=True, train_size=0.8, shuffle_seed=0, transform=None, target_transform=None):\n",
        "        self.raw_df = pd.read_csv(data_csv_filepath)\n",
        "\n",
        "        # From https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
        "        # Shuffles the dataframe\n",
        "        self.raw_df = self.raw_df.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)\n",
        "\n",
        "        # Builts the features and labels arrays\n",
        "        # Each position represents the name of the symptom in the corresponding position in symptoms array\n",
        "        # That means, len(self.symptoms_classes) == self.symptoms.shape[-1]\n",
        "        self.symptoms_classes = self.raw_df.drop(columns=['id', 'prognosis']).columns.to_numpy(dtype='U')\n",
        "        self.symptoms = self.raw_df.drop(columns=['id', 'prognosis']).to_numpy(dtype='bool')\n",
        "        # One-hot encoding of prognosis column\n",
        "        self.raw_output = self.raw_df['prognosis']\n",
        "        one_hot_encoding = pd.get_dummies(self.raw_output, columns = ['prognosis'])\n",
        "        self.labels_classes = one_hot_encoding.columns.to_numpy(dtype='U')\n",
        "        self.labels = one_hot_encoding.to_numpy(dtype='bool')\n",
        "\n",
        "        # Checks that it's between 0 and 1, representing the % of the entire data that is training data\n",
        "        assert 0 <= train_size <= 1\n",
        "\n",
        "        # Partitions the data accordingly\n",
        "        if train:\n",
        "            self.symptoms = self.symptoms[:int(len(self.symptoms) * train_size)]\n",
        "        else:\n",
        "            self.symptoms = self.symptoms[int(len(self.symptoms) * train_size):]\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.symptoms)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        symptoms = tensor(self.symptoms[idx])\n",
        "        label = tensor(self.labels[idx])\n",
        "        if self.transform:\n",
        "            symptoms = self.transform(symptoms)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return symptoms, label\n",
        "\n",
        "    # From https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
        "    def shuffle(self, shuffle_seed=0):\n",
        "        np.random.seed(shuffle_seed)\n",
        "        p = np.random.permutation(len(self.symptoms))\n",
        "        self.symptoms = self.symptoms[p]\n",
        "        self.labels = self.labels[p]\n",
        "        self.symptoms_classes = self.symptoms_classes[p]\n",
        "        self.labels_classes = self.labels_classes[p]\n",
        "\n",
        "\n",
        "class CustomPrognosisDataLoader(DataLoader):\n",
        "    def __init__(self, data, batch_size, shuffle=False, shuffle_seed=0):\n",
        "        if shuffle:\n",
        "            data.shuffle(shuffle_seed)\n",
        "\n",
        "        self.batches = [(\n",
        "            # data\n",
        "            stack([tensor(data[j][0]) for j in range(i, min(i + batch_size, len(data)))]),\n",
        "            # labels\n",
        "            stack([tensor(data[j][1]) for j in range(i, min(i + batch_size, len(data)))])\n",
        "            )\n",
        "        for i in range(0, len(data), batch_size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, itx):\n",
        "        return self.batches[itx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.batches:\n",
        "            yield batch\n",
        "\n",
        "# TEST\n",
        "# data2.csv should be in the current working directory\n",
        "data_json_filepath = os.path.join(os.getcwd(), 'data2.csv')\n",
        "\n",
        "training_data = CustomPrognosisDataset(\n",
        "    data_json_filepath,\n",
        "    train=True\n",
        ")\n",
        "\n",
        "test_data = CustomPrognosisDataset(\n",
        "    data_json_filepath,\n",
        "    train=False\n",
        ")\n",
        "\n",
        "train_dataloader = CustomPrognosisDataLoader(training_data, batch_size=64, shuffle=False)\n",
        "test_dataloader = CustomPrognosisDataLoader(test_data, batch_size=64, shuffle=False)\n",
        "f, l = next(iter(train_dataloader))\n",
        "print(f\"\\nFeature batch shape: {f.size()}\\n\")\n",
        "print(f\"Labels batch shape: {l.size()}\\n\")"
      ],
      "metadata": {
        "id": "hTImQRjEMC0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6baf8fef-96d4-4eb0-c1ff-f4cccf3cb3b6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature batch shape: torch.Size([64, 64])\n",
            "\n",
            "Labels batch shape: torch.Size([64, 11])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-3dcc995c3a97>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  stack([tensor(data[j][0]) for j in range(i, min(i + batch_size, len(data)))]),\n",
            "<ipython-input-72-3dcc995c3a97>:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  stack([tensor(data[j][1]) for j in range(i, min(i + batch_size, len(data)))])\n"
          ]
        }
      ]
    }
  ]
}