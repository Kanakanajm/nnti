\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{parskip}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{8}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: \\ Camilo Mart√≠nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{Universal Approximation Theorem}
    \subsection*{(a)}
    It states that for all continuous functions in $\mathbb{R}^n$,
    there exist a feed forward neural network with at least one hidden layer with unbounded and continuous activation functions
    and a linear ouput layer which can approximate such function.
    \subsection*{(b)}
    No, only FNNs with unbouned activation functions have this property

    \subsection*{(c)}
    \subsubsection*{(i)}
    Because FNNs can overfit to the training data and it may not give answer

    \subsubsection*{(ii)}
    Because FNN with one hidden layer may contains a large number of nodes.
\end{homeworkProblem}

\begin{homeworkProblem}{Regularization}
    \subsection*{(a)}
    \newcommand*{\evep}[1]{\mathbb{E}_\epsilon \left[#1\right]}
    \newcommand*{\avgm}{\frac{1}{m}\sum_{i=1}^{m}}
    To avoid confusion on indicies, we rewrite $\hat{y}$ as $\hat{y}(x_i, w) = w_0 + w^Tx_i$.
    We have such derivation
    \begin{align*}
        \evep{J(w; \{x_i+\epsilon_i\}^m_{i=1}, y)}    &=  \evep{\avgm (y_i - w_0 - w^T(x_i + \epsilon_i))^2}\\
                                                &=  \evep{\avgm (y_i - w_0 - w^Tx_i - w^T\epsilon_i)^2}\\
                                                &=  \evep{\avgm ((y_i - \hat{y}(x_i, w)) - w^T\epsilon_i)^2}\\
                                                &=  \evep{\avgm ((y_i - \hat{y}(x_i, w))^2 - 2(y_i - \hat{y}(x_i, w))(w^T\epsilon_i) +  (w^T\epsilon_i)^2)}\\
                                                &=  \evep{\avgm ((y_i - \hat{y}(x_i, w))^2)} - \evep{\avgm 2(y_i - \hat{y}(x_i, w))(w^T\epsilon_i)} +  \evep{\avgm((w^T\epsilon_i)^2)}\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2) - \avgm 2(y_i - \hat{y}(x_i, w))(w^T\evep{\epsilon_i}) +  \evep{\avgm (w^T\epsilon_iw^T\epsilon_i)}\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2) - \avgm 2(y_i - \hat{y}(x_i, w))(w^T\evep{\epsilon_i}) +  \evep{\avgm (w^T\epsilon_i^T\epsilon_iw)}\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2) - \avgm 2(y_i - \hat{y}(x_i, w))(w^T\evep{\epsilon_i}) +  \avgm (w^T\evep{\epsilon_i^T\epsilon_i}w)\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2) - \avgm 2(y_i - \hat{y}(x_i, w))(w^T \cdot 0) +  \avgm (w^T\sigma^2Iw)\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2) +  \avgm (\sigma^2w^TIw)\\
                                                &=  \avgm ((y_i - \hat{y}(x_i, w))^2 + \sigma^2w^Tw)\\
    \end{align*}
    \subsection*{(b)}
    \begin{align*}
        \nabla_w\tilde{J}  &=  \frac{\lambda}{2} \cdot 2w + \nabla_w J\\
                           &=  \lambda w + \nabla_w J\\
    \end{align*}
    Thus we have the weight update rule for the reularized loss
    \begin{align*}
        w_{t+1} &=  w_t - \eta(\lambda w_t + \nabla_w J)\\
                &=  (1 - \eta\lambda) w_t - \eta\nabla_w J
    \end{align*}
    One can observe that before substracting the gradient $w_t$ is reduced by a constant $\eta\lambda$
\end{homeworkProblem}

\end{document}