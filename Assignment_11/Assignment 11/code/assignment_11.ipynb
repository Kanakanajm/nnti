{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4.1 Design your RNN (4 points)\n",
    "\n",
    "**Note: Use the SIC Cluster for this task**\n",
    "\n",
    "\n",
    "Please create a ```solution.py``` file where you define the following:\n",
    "\n",
    "\n",
    "1. A ```function``` where you use pytorch's Dataset and Dataloader class, and it should return you the desired split for the dataset. The function should have ```split``` as one of its argument and the call to Dataset class should respect this argument. You will manually need to download the dataset first. The desired role of function is as follows:\n",
    "    - Use the ```Large Movie Review Dataset``` dataset. [Link](https://ai.stanford.edu/~amaas/data/sentiment)\n",
    "    - Create Dataset object for different splits\n",
    "    - Computers don't work with natural language, so we have to convert it to some sort of numbers. One such idea would be to use GloVe embeddings for the conversion. Depending on how you choose to do this, you might also have to take care of padding. **Note:** We encourage using the 300d GloVe embeddings.\n",
    "    - Returns the Dataloader object for specified split\n",
    "    - **(Optional)** Try one-hot encoding in-place of GloVe to see how big of an improvement GloVe was for embedding space. There are other (possible but not recommended) ways to do embeddings, such as get POS tags for each word or use a dictionary to define polarity for each word.\n",
    "    \n",
    "2. Multiple ```class``` for your implementation of your networks which does the following:\n",
    "    - Define a RNN class with appropriate layers and hyperparameters\n",
    "    - Define a LSTM class with appropriate layers and hyperparameters\n",
    "    - **(Optionally)** Implement Bi-LSTM, Bi-RNN, Bi-GRU and do a comparison with the one-directional implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4.2 (Bonus) Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This exercise is mostly devoted to the Transformer model which will be described during lecture on 30th of January."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will be using Multi-Head Attention to solve a toy exercise in sequence modeling. The concept of Multi-Head Attention is taken from a famous paper called [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762), which introduced Transformer model. Please read the paper carefully and answer the questions below. Understanding the concepts described in this paper will help understanding many modern models in the Neural Networks field and it's also necessary if choose to pick NLP project. \n",
    "\n",
    "If you have troubles understanding the paper you can read [this blog post](https://jalammar.github.io/illustrated-transformer/) first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The biggest benefit of using Transformers instead of RNN and convolution based models is the possibility to paralllelize computations during training. Why parallelization is not possible with RNN and Convolution based models for sequence processing, but possible with Transformers? *Note*: parallelization can be applied only to the Encoder part of the Trasnformer. (0.5 points)\n",
    "2. In expaining the concept of self attention the paper mentions 3 matrices `Q`, `K` and `V` which serve as an input to self-attention mechanism sublayer. Explain how these matrices are computed in the encoder and in the decoder. What role each of these matrices play? (1 point)  \n",
    "3. How is Multi-Head Attention better than Single-Head Attention? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task description\n",
    "Given an input sequence `XY[0-5]+` (two digits X and Y followed by the sequence of digits in the range from 0 to 5 inclusive), the task is to count the number of occurrences of X and Y in the remaining substring and then calculate the difference #X - #Y.\n",
    "\n",
    "Example:  \n",
    "Input: `1214211`  \n",
    "Output: `2`  \n",
    "Explanation: there are 3 `1`'s and 1 `2` in the sequence `14211`, `3-1=2`  \n",
    "  \n",
    "The model must learn this relationship between the symbols of the sequence and predict the output. This task can be solved with a multi-head attention network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11773aa10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 5\n",
    "VOCAB_SIZE = 6\n",
    "NUM_TRAINING_STEPS = 25000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation function\n",
    "Fill the code to calculate the ground truth outpu for the random sequence and store it in `gts`.    \n",
    "\n",
    "Why are we offseting the ground truth? In other words, why do we need grouth truth to be non-negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data samples as described at the beginning of the\n",
    "# script\n",
    "def get_data_sample(batch_size=1):\n",
    "    random_seq = torch.randint(low=0, high=VOCAB_SIZE - 1,\n",
    "                               size=[batch_size, SEQ_LEN + 2])\n",
    "    \n",
    "    ############################################################################\n",
    "    # TODO: Calculate the ground truth output for the random sequence and store\n",
    "    # it in 'gts'.\n",
    "    ############################################################################\n",
    "    gts = gts.squeeze()\n",
    "\n",
    "    # Ensure that GT is non-negative\n",
    "    ############################################################################\n",
    "    # TODO: Why is this needed?\n",
    "    ############################################################################\n",
    "    gts += SEQ_LEN\n",
    "    return random_seq, gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_sample(batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "Implement a naive version of Attention mechanism in the following class. Please do not derive from the given structure. If you have ideas about how to optimize the implementation you can however note them in a comment or provide an additional implementation.  \n",
    "In your implementation refer to Section 3.2.1 and Figure 2 (left) in the paper. Keep the parameters to the forward pass trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # q, k, and v are batch-first\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "Implement Multi-Head Attention mechanism on top of the Single-Head Attention mechanism in the following class. Please do not derive from the given structure. If you have ideas about how to optimize the implementation you can however note them in a comment or provide an additional implementation.  \n",
    "In your implementation refer to Section 3.2.2 and Figure 2 (right) in the paper. Keep the parameters to the forward pass trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_r = self.embed_dim // self.num_heads   # to evenly split q, k, and v across heads.\n",
    "        self.dotatt = Attention()\n",
    "\n",
    "        self.q_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.final_linear_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        # xavier initialization for linear layer weights\n",
    "        nn.init.xavier_uniform_(self.q_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.final_linear_proj.weight)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # q, k, and v are batch-first\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Implement multi-head attention as described in Section 3.2.2\n",
    "        # of the paper.\n",
    "        ########################################################################\n",
    "        # shapes of q, k, v are [bsize, SEQ_LEN + 2, hidden_dim]\n",
    "        bsize = k.shape[0]\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Layer\n",
    "Implement the Encoding Layer of the network.  \n",
    "Refer the following figure from the paper for the architecture of the Encoding layer (left part of the figure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/eAKQu.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.stack.imgur.com/eAKQu.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(embed_dim=num_hidden, num_heads=num_heads)\n",
    "        # TODO: add necessary member variables\n",
    "    def forward(self, x):\n",
    "        x = self.att(x, x, x)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network definition\n",
    "Implement the forward pass of the complete network.\n",
    "The network must do the following:\n",
    "1. calculate embeddings of the input (with the size equal to `num_hidden`)\n",
    "2. perform positional encoding\n",
    "3. perform forward pass of a single Encoding layer\n",
    "4. perform forward pass of a single Decoder layer\n",
    "5. apply fully connected layer on the output\n",
    "\n",
    "Because we are dealing with quite simple task, the whole Decoder layer can be replaced with a single MultiHeadAttention block. Since our task is not sequence to sequence, but rather the classification of a sequence, the query (`Q` matrix) for the MultiHeadAttention block can be another learnable parameter (`nn.Parameter`) instead of processed output embedding.\n",
    "\n",
    "In the forward pass we must add a (trainable) positional encoding of our input embedding. Why is this needed? Can you think of another similar task where the positional encoding would not be necessary?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_encoding_layers=1, num_hidden=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        q = torch.empty([1, num_hidden])\n",
    "        nn.init.normal_(q)\n",
    "        self.q = nn.Parameter(q, requires_grad=True)\n",
    "        \n",
    "        # TODO: implement\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Don't edit the following 2 cells. They must run without errors if you implemented the model correctly.  \n",
    "The model should converge to nearly 100% accuracy after ~4.5k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate network, loss function and optimizer\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "for i in range(NUM_TRAINING_STEPS):\n",
    "    inputs, labels = get_data_sample(BATCH_SIZE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    accuracy = (torch.argmax(outputs, axis=-1) == labels).float().mean()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('[%d/%d] loss: %.3f, accuracy: %.3f' %\n",
    "              (i , NUM_TRAINING_STEPS - 1, loss.item(), accuracy.item()))\n",
    "    if i == NUM_TRAINING_STEPS - 1:\n",
    "        print('Final accuracy: %.3f, expected %.3f' %\n",
    "              (accuracy.item(), 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly analyze the results you get. Does the model learn the underlying pattern in all the sequences? How can we improve the results / speed up the training process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
