\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

\usepackage{biblatex} 
\addbibresource{bibliography.bib} % Import the bibliography file

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{4}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam \\ Camilo Martínez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Bias and Variance}

\subsection*{(a)}
If $\hat{f}$ is the predicted function by our linear regression model, then the variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$. But ideally the estimate for $f$ should not vary too much between training sets. In other words, high variance implies that small changes in the training data can result in large changes in our predicted function $\hat{f}$. In general, more flexible statistical methods have higher variance. On the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between $Y$ and a set of predictors $X_i$. One could say that the model is biased to consider that given a set of $x_i$ is proportional or linearly related to a set of outputs $y_i$. Generally, more flexible methods result in less bias. More flexible methods result in an increase in variance and a decrease in bias. Bias indicates how well the model predicts and Variance is a measurement of how similar the models trained with different training set behaves. A complex model results in a low bias and a high variance and a simple model results in a high bias and a low variance \cite{gareth2023intro}.

\subsection*{(b)}
Considering the concepts introduced in the explanation above, overfitting means low bias but high variance. On the other hand, underfitting means high bias but low variance.

\subsection*{(c)}
To show that the given equation holds, we have to consider the mathematical definition of bias and variance of a function $f$, $Bias(f)$ and $Var(f)$ \cite{jordan2006section1} \cite{jordan2006section2}:
\[
Bias(f(x)) = E[f(x)] - f(x)
\]
\[
Var(f(x)) = E[(f(x) - E[f(x)])^2]
\]
With these definitions and using the properties of the expected value of a function, represented by $E[f(x)]$ we can perform the following derivation:
\newcommand{\fh}{\hat{f}}
\begin{align*}
MSE(y, \fh) &= E[(y - \fh(x_0))^2)]\\
&= E[y^2 - 2y\fh+\fh^2]\\
&=E[y^2] - 2E[y\fh] + E[\fh^2]\\
&=E[(f+\varepsilon)^2] - 2E[(f+\varepsilon)\fh] + E[\fh^2]\\
&=E[f^2]+ 2E[f]E[\varepsilon] + E[\varepsilon^2] - 2(E[f\fh]+ E[\varepsilon]E[\fh]) + E[\fh^2]\\
&=f^2+2f\cdot0 + Var(\varepsilon) - 2E[f]E[\fh] - 2\cdot0\cdot E[\fh] + E[\fh^2]\\
&=f^2+Var(\varepsilon) - 2fE[\fh] + E[\fh^2]\\
&=f^2+Var(\varepsilon) - 2fE[\fh] + E[\fh^2] + E[\fh]^2 - E[\fh]^2\\
&=(f^2 - 2fE[\fh] + E[\fh]^2) + (E[\fh^2] - E[\fh]^2)+Var(\varepsilon)\\
&=(f-E[\fh])^2+ (E[\fh^2] - E[\fh]^2)+Var(\varepsilon)\\
&=Bias(\fh)^2+ Var(\fh) +Var(\varepsilon)
\end{align*}
That is, the MSE can be decomposed into the sum of squared bias, variance, and irreducible error. This decomposition is the bias-variance tradeoff. It means that in order to minimize the MSE, we need to select a statistical learning method that simultaneously achieves that our $\hat{f}$ has low variance and low bias (it is a tradeoff ultimately). Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative \cite{gareth2023intro}.

\subsection*{(d)}
When the training set size goes up, the variance goes down. This can be intuitively derived, since more data generally means that our models can get more robust, since the model now has more data to work with and derive the intricacies in-between. Mathematically speaking, the variance is inversely proportional to the training size $N$. On the other hand, bias will remain the same even if the training set size increases. This is because bias is inherently correlated with the model's degrees of freedom or complexity and is therefore not related to the training size. We see this in the plot of the final exercise of the practical problem. Nevertheless, practically speaking, it is still true that a small training set limits the optimal capacity of our models, meaning we cannot train more complex models, even if we would like to do so. This was seen in the slide 30/50 of Chapter 4. 

\end{homeworkProblem}

\begin{homeworkProblem}{- Maximum Likelihood Estimate (MLE)}

    \subsection*{(a)} 
    Let the output variable \(y = y_1, ..., y_m\) consist of \(m\) i.i.d. normal variables and has likelihood
    \begin{equation}\label{first}
    p(\bm{y} | \bm{X}, \bm{w}) = \prod_{m=1}^M {\mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2)}
    \end{equation}
    Where 
    \[
    \mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)
    \]
    To maximize (\ref{first}), we can take the \(\log{}\) which makes it simpler to work with. This is called the log-likelihood and is defined as follows:
    \begin{equation}\label{second}
    \log{(p(\bm{y} | \bm{X}, \bm{w}))} = \log{\biggl(\prod_{m=1}^M {\mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2)}\biggr)}
    \end{equation}
    From there, we take advantage of the properties of the \(\log\) function, mainly \(\log{(ab)} = \log{a} + \log{b}\)
    \[
    \begin{split}
    \log{(p(\bm{y} | \bm{X}, \bm{w}))} &= \sum_{m=1}^M {\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}} \exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)\biggr)}} \\
    &= \sum_{m=1}^M {\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}}\biggl)}} +  \sum_{m=1}^M {\log{\biggl(\exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)\biggr)}} \\
    &= M\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}}\biggl)} + \sum_{m=1}^M {-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2} \\
    &= -M\log{(\sigma \sqrt{2\pi})} - \frac{1}{2\sigma^2} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \end{split}
    \]
    Finally, we arrive at the following expression
    \begin{equation}\label{third}
        \begin{split} 
        \log{(p(\bm{y} | \bm{X}, \bm{w}))} &= -M\log{(\sigma)} - \frac{M}{2} \log{(2\pi)} - \frac{1}{2\sigma^2} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
        \end{split}
    \end{equation}
    Since \(-M\log{(\sigma)} - \frac{M}{2} \log{(2\pi)}\) as well as  \(\frac{1}{2\sigma^2}\) are just constants, we only need to maximize the last term that involves the sum. Moreover, maximizing a negative value is the same as minimizing its positive counterpart. Thus, the entire problem becomes minimizing the following expression
    \[
    \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \]
    From there, we can introduce a new constant \(\frac{1}{M}\) which does not affect the minimization process at all, and we get an expression that is the Mean Squared Error, for which we derived on Assignment 3 Exercise 3.1.c the optimal weight vector \(\bm{w}\) 
    \[
    \mathrm{MSE} = \frac{1}{M} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \]
    This proves that a linear regression procedure that consists of minimizing the MSE can be justified as a Maximum-Likelihood procedure, which was our starting point.
    
    \subsection*{(b)}
    Assuming a standard normal prior on the weights \(\bm{w}\) of the form \(\mathcal{N}(\bm{w} \, | \, 0, \frac{1}{\lambda}\bm{I})\), where \(\lambda\) is just a constant that defines the precision of the distribution, we can express the likelihood function as follows
    \[
    p(\bm{w} \, | \, \lambda) = \prod_{m=1}^M {\mathcal{N}\biggl(\bm{w} \, | \, 0, \frac{1}{\lambda}\bm{I}\biggr)} = \biggl(\frac{\lambda}{2\pi}\biggr)^{M/2} \exp\biggl(-\frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}\biggr)
    \]
    Where \(M\) is the total number of elements in the \(\bm{w}\) vector. Using Bayes’ theorem, the posterior distribution for \(\bm{w}\) is proportional to the product of the prior distribution and the likelihood function, that is
    \begin{equation}\label{fourth}
    p(\bm{w} \, | \, \bm{X}, \bm{y}, \lambda) \propto p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda) \, p(\bm{w} \, | \, \lambda)
    \end{equation}
    Where \(p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda)\) is the same one we introduced in the previous exercise, with an additional parameter \(\lambda\). Now, to determine the weight vector \(\bm{w}\), we need to find the most probable value of \(\bm{w}\) given the data, in other words by maximizing the posterior distribution given by (\ref{fourth}). This technique is called maximum posterior, or simply MAP. As we have already explained in previous exercises, we can approach this problem by taking the \(\log{}\) function and maximize that instead. That is, our expression to maximize becomes
    \begin{equation}\label{fith}
    \log{(p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda) \, p(\bm{w} \, | \, \lambda))} = \log{p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda)} + \log{p(\bm{w} \, | \, \lambda)}
    \end{equation}
    In (\ref{fith}), we can identify that the first term is the same as the one in the previous exercise. And we already proved that maximizing that term is the same as minimizing \(\sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}\) or any constant multiplied by this term. Note that this expression does not depend on \(\sigma\), which we defined as our precision parameter \(\lambda\). On the other hand, maximizing the second term \(\log{p(\bm{w} \, | \, \lambda)}\) is the same as maximizing \(-\frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}\), since \((\frac{\lambda}{2\pi})^{M/2}\) is just a constant. And that in turn is the same as minimizing its negative counterpart. In summary, the entire problem becomes minimizing the following expression
    \begin{equation}\label{sixth}
    \mathcal{L}(\bm{w}) = \frac{1}{2}\sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2} + \frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}
    \end{equation}
    Note that we multiplied the first term by \(1/2\) which does not have any impact on the minimization process, since it is just a constant. Finally, we recognize that the resulting expression in (\ref{sixth}) is the \(L_2\)-regularized least squares loss. Thus we see that maximizing the posterior distribution or MAP procedure is equivalent to minimizing the least squares criterion with \((L_2\-\)-)regularization, also known as ridge regression in the Statistics literature, with a regularization parameter given by \(\lambda\), by assuming a standard normal prior on the weights.

\end{homeworkProblem}

\begin{homeworkProblem}{- Bias-Variance Trade-Off Exploration}

See attached .ipynb solution in .zip file.

\end{homeworkProblem}

\printbibliography

\end{document}