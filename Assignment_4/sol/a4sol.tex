\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{4}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: Camilo Martínez 7057573, Honglu Ma 7055053}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Bias and Variance}

\subsection*{a)}
Bias indicates how well the model predicts and Variance is a measurement of how similar the models trained with different training set behaves. A complex model results in a low bias and a high variance and a simple model results in a high bias and a low variance.

\subsection*{b)}
Overfitting means low bias but high variance.
Underfitting means high bias but low variance.

\subsection*{c)}
\newcommand{\fh}{\hat{f}}
\begin{align*}
MSE(y, \fh) &= E[(y - \fh(x_0))^2)]\\
&= E[y^2 - 2y\fh+\fh^2]\\
&=E[y^2] - 2E[y\fh] + E[\fh^2]\\
&=E[(f+\varepsilon)^2] - 2E[(f+\varepsilon)\fh] + E[\fh^2]\\
&=E[f^2]+ 2E[f]E[\varepsilon] + E[\varepsilon^2] - 2(E[f\fh]+ E[\varepsilon]E[\fh]) + E[\fh^2]\\
&=f^2+2f\cdot0 + Var(\varepsilon) - 2E[f]E[\fh] - 2\cdot0\cdot E[\fh] + E[\fh^2]\\
&=f^2+Var(\varepsilon) - 2fE[\fh] + E[\fh^2]\\
&=f^2+Var(\varepsilon) - 2fE[\fh] + E[\fh^2] + E[\fh]^2 - E[\fh]^2\\
&=(f^2 - 2fE[\fh] + E[\fh]^2) + (E[\fh^2] - E[\fh]^2)+Var(\varepsilon)\\
&=(f-E[\fh])^2+ (E[\fh^2] - E[\fh]^2)+Var(\varepsilon)\\
&=Bias(\fh)^2+ Var(\fh) +Var(\varepsilon)\\
\end{align*}

\subsection*{d)}
When the training set size goes up, the variance goes down but the bias stays the same
but when the training set is really small, increasing the training set size can increase the bias as well.

\end{homeworkProblem}

\begin{homeworkProblem}{- Maximum Likelihood Estimate (MLE)}

    \subsection*{(a)} 
    Let the output variable \(y = y_1, ..., y_m\) consist of \(m\) i.i.d. normal variables and has likelihood
    \begin{equation}\label{first}
    p(\bm{y} | \bm{X}, \bm{w}) = \prod_{m=1}^M {\mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2)}
    \end{equation}
    Where 
    \[
    \mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)
    \]
    To maximize (\ref{first}), we can take the \(\log{}\) which makes it simpler to work with. This is called the log-likelihood and is defined as follows:
    \begin{equation}\label{second}
    \log{(p(\bm{y} | \bm{X}, \bm{w}))} = \log{\biggl(\prod_{m=1}^M {\mathcal{N}(\bm{y_m} | \bm{w}^\mathsf{T} \bm{x_m}, \sigma^2)}\biggr)}
    \end{equation}
    From there, we take advantage of the properties of the \(\log\) function, mainly \(\log{(ab)} = \log{a} + \log{b}\)
    \[
    \begin{split}
    \log{(p(\bm{y} | \bm{X}, \bm{w}))} &= \sum_{m=1}^M {\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}} \exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)\biggr)}} \\
    &= \sum_{m=1}^M {\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}}\biggl)}} +  \sum_{m=1}^M {\log{\biggl(\exp\biggl(-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2 \biggr)\biggr)}} \\
    &= M\log{\biggl(\frac{1}{\sigma \sqrt{2\pi}}\biggl)} + \sum_{m=1}^M {-\frac{1}{2\sigma^2}(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2} \\
    &= -M\log{(\sigma \sqrt{2\pi})} - \frac{1}{2\sigma^2} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \end{split}
    \]
    Finally, we arrive at the following expression
    \begin{equation}\label{third}
        \begin{split} 
        \log{(p(\bm{y} | \bm{X}, \bm{w}))} &= -M\log{(\sigma)} - \frac{M}{2} \log{(2\pi)} - \frac{1}{2\sigma^2} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
        \end{split}
    \end{equation}
    Since \(-M\log{(\sigma)} - \frac{M}{2} \log{(2\pi)}\) as well as  \(\frac{1}{2\sigma^2}\) are just constants, we only need to maximize the last term that involves the sum. Moreover, maximizing a negative value is the same as minimizing its positive counterpart. Thus, the entire problem becomes minimizing the following expression
    \[
    \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \]
    From there, we can introduce a new constant \(\frac{1}{M}\) which does not affect the minimization process at all, and we get an expression that is the Mean Squared Error, for which we derived on Assignment 3 Exercise 3.1.c the optimal weight vector \(\bm{w}\) 
    \[
    \mathrm{MSE} = \frac{1}{M} \sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}
    \]
    This proves that a linear regression procedure that consists of minimizing the MSE can be justified as a Maximum-Likelihood procedure, which was our starting point.
    
    \subsection*{(b)}
    Assuming a standard normal prior on the weights \(\bm{w}\) of the form \(\mathcal{N}(\bm{w} \, | \, 0, \frac{1}{\lambda}\bm{I})\), where \(\lambda\) is just a constant that defines the precision of the distribution, we can express the likelihood function as follows
    \[
    p(\bm{w} \, | \, \lambda) = \prod_{m=1}^M {\mathcal{N}\biggl(\bm{w} \, | \, 0, \frac{1}{\lambda}\bm{I}\biggr)} = \biggl(\frac{\lambda}{2\pi}\biggr)^{M/2} \exp\biggl(-\frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}\biggr)
    \]
    Where \(M\) is the total number of elements in the \(\bm{w}\) vector. Using Bayes’ theorem, the posterior distribution for \(\bm{w}\) is proportional to the product of the prior distribution and the likelihood function, that is
    \begin{equation}\label{fourth}
    p(\bm{w} \, | \, \bm{X}, \bm{y}, \lambda) \propto p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda) \, p(\bm{w} \, | \, \lambda)
    \end{equation}
    Where \(p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda)\) is the same one we introduced in the previous exercise, with an additional parameter \(\lambda\). Now, to determine the weight vector \(\bm{w}\), we need to find the most probable value of \(\bm{w}\) given the data, in other words by maximizing the posterior distribution given by (\ref{fourth}). This technique is called maximum posterior, or simply MAP. As we have already explained in previous exercises, we can approach this problem by taking the \(\log{}\) function and maximize that instead. That is, our expression to maximize becomes
    \begin{equation}\label{fith}
    \log{(p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda) \, p(\bm{w} \, | \, \lambda))} = \log{p(\bm{y} \, | \, \bm{X}, \bm{w}, \lambda)} + \log{p(\bm{w} \, | \, \lambda)}
    \end{equation}
    In (\ref{fith}), we can identify that the first term is the same as the one in the previous exercise. And we already proved that maximizing that term is the same as minimizing \(\sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2}\) or any constant multiplied by this term. Note that this expression does not depend on \(\sigma\), which we defined as our precision parameter \(\lambda\). On the other hand, maximizing the second term \(\log{p(\bm{w} \, | \, \lambda)}\) is the same as maximizing \(-\frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}\), since \((\frac{\lambda}{2\pi})^{M/2}\) is just a constant. And that in turn is the same as minimizing its negative counterpart. In summary, the entire problem becomes minimizing the following expression
    \begin{equation}\label{sixth}
    \mathcal{L}(\bm{w}) = \frac{1}{2}\sum_{m=1}^M {(\bm{y_m} - \bm{w}^\mathsf{T} \bm{x_m})^2} + \frac{\lambda}{2}\bm{w}^\mathsf{T} \bm{w}
    \end{equation}
    Note that we multiplied the first term by \(1/2\) which does not have any impact on the minimization process, since it is just a constant. Finally, we recognize that the resulting expression in (\ref{sixth}) is the \(L_2\)-regularized least squares loss. Thus we see that maximizing the posterior distribution or MAP procedure is equivalent to minimizing the least squares criterion with \((L_2\-\)-)regularization, also known as ridge regression in the Statistics literature, with a regularization parameter given by \(\lambda\), by assuming a standard normal prior on the weights.

\end{homeworkProblem}

\begin{homeworkProblem}{- Bias-Variance Trade-Off Exploration}

See attached .ipynb solution in .zip file.

\end{homeworkProblem}

\end{document}