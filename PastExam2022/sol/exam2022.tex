\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{changepage}
\usepackage{color}
\usepackage{hyperref}


\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
% \lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{E22}
\newcommand{\hmwkTitle}{Exam 2022}
\newcommand{\hmwkClass}{NNTI}
% \newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{Jackie, Camilo, Kai, Dhimitrios, Hevra}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\newenvironment{homeworkSubsection}{\begin{adjustwidth}{2.5em}{0pt}}{\end{adjustwidth}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\begin{document}

\maketitle

\begin{homeworkProblem}{Short questions}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        (i) is not a good candidate because the derivative is 0 that will make gradient vanish.
        (ii) is not a good candidate because the network is limited to learn only quadratic functions.
        (iii) Uhh.. what is the normal distribution formular doing here?
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        (ii) is correct because a deep network will face a problem called covariance shift
        , that is the variance of the intermediate layer output may change after the weights from the previous layer being updated.
        Batch norm make sure that the variance of such layer so we can introduce a higher learning rate to the network
        thus speeds up the convergence.
    \end{homeworkSubsection}
    \subsection*{(c)}
    \begin{homeworkSubsection}
        Batch Size = 1? The batch norm video from Andrew Ng says batch norm is applied to the intermediate layer output. Confused.
    \end{homeworkSubsection}
    \subsection*{(d)}
    \begin{homeworkSubsection}
        (i) Yes because the kernel depth has to match the input volumn depth

        (ii) Yes, usually one bias per kernel/filter.

        (iii) No because it only affects output dimension.

        (iv) No because it only affects output dimension.

    \end{homeworkSubsection}
    \subsection*{(e)}
    \begin{homeworkSubsection}
        Datasets that are not linearly separable are difficult for a linear classifier to correctly
        classify.
    \end{homeworkSubsection}
    \subsection*{(f)}
    \begin{homeworkSubsection}
        Because it is computationally expensive.
    \end{homeworkSubsection}
    \subsection*{(g)}
    \begin{homeworkSubsection}
        Max pooling can make the kernels pick up feature even if it is rotated.
    \end{homeworkSubsection}
    \subsection*{(h)}
    \begin{homeworkSubsection}
        It is not a good idea because sigmoid's exponential term may be overflow.
    \end{homeworkSubsection}
    \subsection*{\red{(i)}}
    \subsection*{\red{(j)}}
\end{homeworkProblem}
\begin{homeworkProblem}{Principal Component Analysis (PCA)}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        The first assumption is that the dataset are centered at 0
        i.e. the mean is substracted from each data point
        because PCA trys to project the points on the the pca hyperplane (or line)
        so that the variance of the original dataset does not change much
        and the new axes are go through the original.

        The second one is that the dataset is normalized
        because different variables (axes) have different scales.
        Their covariances are not comparable if the data are not under the same scale.
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        The objective function is given by:
        \[
            \underset{D}{\mathrm{argmin}}\underset{i}{\sum}||x^{(i)}-DD^\top x^{(i)}||
        \]
        where matrix $D \in \mathbb{R}^{m\times l}$ and vectors $x^{(i)} \in \mathbb{R}^{m}$.
        We are looking for a matrix $D$ that tranforms $x^{(i)}$ into a lower dimention($l$) space
        and minimizes the reconstruction error.
    \end{homeworkSubsection}
    \subsection*{(c)}
    \subsection*{(d)}
    \begin{homeworkSubsection}
        From A3.2b we know that the objective function can also be written as
        $\underset{D}{\mathrm{argmin}}||X-XDD^\top||_F^2$ and later derived as
        $\underset{D}{\mathrm{argmax}}\sum_{i=1}^{m}D_{.i}^\top X^\top XD_{.i}$
        where $D_{.i}$ is a column of $D$.
        We simplify the problem by treating $D_{.i}$ as a vector $d$ with constraint $d^\top d = 1$:
        \begin{align*}
            \underset{d}{\mathrm{argmax}}d^\top X^\top Xd
            &= \underset{d}{\mathrm{argmax}}(Xd)^2\\
            &= \underset{d}{\mathrm{argmax}}\sum_{i=1}^{n}(d^\top x^{(i)})^2\\
            &= \underset{d}{\mathrm{argmax}}\frac{1}{n}\sum_{i=1}^{n}(d^\top x^{(i)} - 0)^2
        \end{align*}
        One can observe that the formular $\frac{1}{n}\sum_{i=1}^{n}(d^\top x^{(i)} - 0)^2$ is 
        calculating the variance of the points projected onto direction $d$ (note that the mean is 0 from assumption).
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Machine Learning Basics}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        We have the loss function $J(w) = \frac{1}{n}||Xw-y||^2_2 + \lambda w^\top w$.
        We can find the $w$ minimizes $J(W)$ by setting the gradient $\nabla_wJ(w) = 0$.
        \begin{align*}
            \nabla_wJ(w) &= 0\\
            \frac{2}{n}X^\top(Xw-y) + 2\lambda w &= 0\\
            X^\top Xw-X^\top y + n\lambda w &= 0\\
            (X^\top X + n\lambda I)w-X^\top y &= 0\\
            (X^\top X + n\lambda I)w &= X^\top y \\
            w  &=(X^\top X + n\lambda I)^{-1}X^\top y\\
        \end{align*}
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        The equation is the same as $J(w) = \frac{1}{2}||Xw-y||^2_2 + \alpha ||w||_1  + \frac{\lambda}{2}w^Tw$.
        One can observe that there is an L1 norm in the equation.
        The derivative of L1 norm cannot be expressed as a closed form because it depends on the sign of each element in vector $w$.
    \end{homeworkSubsection}
    \subsection*{(c)}
    \begin{homeworkSubsection}
        Now we have the $J(w) = \frac{1}{n}||Xw-y||^2_2 + \lambda w^\top T w$.
        Similarly we solve $\nabla_wJ(w) = 0$.
        \begin{align*}
            \frac{2}{n}X^\top(Xw-y) + \lambda (T + T^\top)w &= 0\\
            X^\top(Xw-y) + \frac{n\lambda}{2} (2T)w &= 0\\
            X^\top Xw- X^\top y + n\lambda Tw &= 0\\
            (X^\top X + n\lambda T)w &= X^\top y\\
            w &= (X^\top X + n\lambda T)^{-1}X^\top y\\
        \end{align*}
    \end{homeworkSubsection}
    \subsection*{(d)}
    \begin{homeworkSubsection}
        We can rewrite $w^\top w$ as $w^\top (ww^\top)^{\gamma-1} w$.
        Since $(ww^\top)^{\gamma-1}$ is symmetric matrix we can reuse the result from (c) and get
        \[
            w = (X^\top X + n\lambda (ww^\top)^{\gamma-1})^{-1}X^\top y\\
        \]
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Feed-Forward Neural Networks}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        A deep nn can be computationally effecient as it can learn the same function with fewer parameters \red{Why?}
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        A saddle point is when the gradient is 0 (derivatives are 0 in both x and y direction)
        but the second derivatives in x and y direction are of different sign.
        As the gradient is 0, it may stop the gradient descent process even though it is not at a minimal.
    \end{homeworkSubsection}
    \subsection*{\red{(c)}}
    \subsection*{(d)}
    \begin{homeworkSubsection}
        MSE (Mean square error) measures the euclidean distance between the true value and the predicted one.
        It can be used for regression task.
        \red{Cross Entropy} can be used for classification task.
    \end{homeworkSubsection}
    \subsection*{(e)}
    \begin{homeworkSubsection}
        Quoted from Assignment 5.2:\\
        \textit{Consider a Neural Network with a single hidden layer consisting of M neurons and tanh
        activation function.For any neuron in the hidden layer, simultaneous change of sign of input
        and output weights from the neuron leads to no change in the output layer therefore producing
        an equivalent transformation.Similarly, for any pair of neurons interchange of input weights
        between the neurons and simultaneous interchange of output weights produces an equivalent
        transformation}
    \end{homeworkSubsection}
    \begin{homeworkSubsection}
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Regularization}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        \begin{align*}
            \theta^{MAP*}   &= \underset{\theta}{\mathrm{argmax}}\;P(\theta|D)\\
                            &= \underset{\theta}{\mathrm{argmax}}\;P(\theta)P(D|\theta)\\
                            &= \underset{\theta}{\mathrm{argmax}}\;\mathrm{log}(P(\theta)P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\mathrm{log}P(\theta) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\mathrm{log}\prod_{i=j}^{M}P(\theta_j) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\sum_{j=1}^{M}\mathrm{log}P(\theta_j) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\sum_{j=1}^{M}\mathrm{log}(\frac{1}{\sigma_1\,\sqrt[]{2\pi}}e^{-\frac{\theta_j^2}{2\sigma_1^2}}) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\sum_{j=1}^{M}(\mathrm{log}(\frac{1}{\sigma_1\,\sqrt[]{2\pi}}) + \mathrm{log}(e^{-\frac{\theta_j^2}{2\sigma_1^2}})) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\sum_{j=1}^{M}(\mathrm{log}(\frac{1}{\sigma_1\,\sqrt[]{2\pi}}) -\frac{\theta_j^2}{2\sigma_1^2}) + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(\sum_{j=1}^{M}\mathrm{log}(\frac{1}{\sigma_1\,\sqrt[]{2\pi}}) -\sum_{j=1}^{M}\frac{\theta_j^2}{2\sigma_1^2} + \mathrm{log}P(D|\theta))\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(-\sum_{j=1}^{M}\frac{\theta_j^2}{2\sigma_1^2} + \mathrm{log}P(D|\theta))\\
                            \intertext{similarly for $P(D|\theta)$}\\
                            &= \underset{\theta}{\mathrm{argmax}}\;(-\sum_{j=1}^{M}\frac{\theta_j^2}{2\sigma_1^2} -\sum_{i=1}^{N}\frac{(y_i - f(x_i, \theta))^2}{2\sigma_2^2})\\
                            &= \underset{\theta}{\mathrm{argmin}}\;(\frac{\sigma_2^2}{\sigma_1^2}\sum_{j=1}^{M}\theta_j^2 + \sum_{i=1}^{N}((y_i - f(x_i, \theta))^2))\\
        \end{align*}
        One can observe that $\sum_{j=1}^{M}\theta_j^2$ corresponds to the $w^\top w$ term in the L2 regularization.

        Check out this \href{http://shaofanlai.com/post/79}{article} for more details.
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        We have now the loss function $\hat{J}(w) = \frac{1}{n}||Xw-y||^2_2 + \lambda w^\top w$ with L2 regularization
        which can be rewritten in terms of the normal loss function $\hat{J}(w) = J(w) + \lambda w^\top w$.
        We calculate its gradient:
        \begin{align*}
            \nabla_w\hat{J}(w) &= \nabla_w(J(w) + \lambda w^\top w)\\
            &= \nabla_wJ(w) + \nabla_w(\lambda w^\top w)\\
            &= \nabla_wJ(w) + \lambda \nabla_w(w^\top w)\\
            &= \nabla_wJ(w) + \lambda 2w\\
        \end{align*}
        and the weights update step becomes:
        \begin{align*}
            w_{t+1} \longleftarrow w_t - \alpha(\nabla_w\hat{J}(w)) &= w_t - \alpha(\nabla_wJ(w) + 2\lambda w)\\
            &= w_t - \alpha\nabla_wJ(w) - 2\alpha\lambda w\\
            &= (1-2\alpha\lambda)w_t - \alpha\nabla_wJ(w)\\
        \end{align*}
        One can observe that the weights are shrinked by a factor of $(1-2\alpha\lambda)$.

    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Back propagation}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        \[
            y = [\frac{27}{106}, \frac{1}{106}, \frac{2}{53}, \frac{27}{53}]
        \]
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        \begin{center}
            \begin{tikzpicture}[scale=0.2]
            \tikzstyle{every node}+=[inner sep=0pt]
            \draw [black] (14.9,-8.7) circle (3);
            \draw (14.9,-8.7) node {$z_1$};
            \draw [black] (14.9,-27.5) circle (3);
            \draw (14.9,-27.5) node {$z_2$};
            \draw [black] (29.2,-18) circle (3);
            \draw (29.2,-18) node {$+$};
            \draw (29.2,-15) node[above] {$r$};
            \draw [black] (46.6,-8.7) circle (3);
            \draw (46.6,-8.7) node {$\div$};
            \draw (46.6,-5.7) node[above] {$y_1$};
            \draw [black] (47.5,-27.5) circle (3);
            \draw (47.5,-27.5) node {$\div$};
            \draw (47.5,-24.5) node[above] {$y_2$};
            \draw [black] (17.4,-25.84) -- (26.7,-19.66);
            \fill [black] (26.7,-19.66) -- (25.76,-19.69) -- (26.31,-20.52);
            \draw [black] (17.41,-10.34) -- (26.69,-16.36);
            \fill [black] (26.69,-16.36) -- (26.29,-15.51) -- (25.74,-16.35);
            \draw [black] (17.9,-8.7) -- (43.6,-8.7);
            \fill [black] (43.6,-8.7) -- (42.8,-8.2) -- (42.8,-9.2);
            \draw [black] (17.9,-27.5) -- (44.5,-27.5);
            \fill [black] (44.5,-27.5) -- (43.7,-27) -- (43.7,-28);
            \draw [black] (31.86,-19.38) -- (44.84,-26.12);
            \fill [black] (44.84,-26.12) -- (44.36,-25.31) -- (43.9,-26.19);
            \draw [black] (31.85,-16.59) -- (43.95,-10.11);
            \fill [black] (43.95,-10.11) -- (43.01,-10.05) -- (43.48,-10.93);
            \end{tikzpicture}
        \end{center}
    \end{homeworkSubsection}
    \subsection*{(c)}
    \begin{homeworkSubsection}
        (i)\,We have these local gradients:
        \[
            \frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial y_1}
            \cdot\frac{\partial y_1}{\partial r}
            \cdot\frac{\partial r}{\partial z_1}
            = \frac{\partial L}{\partial y_1}\cdot\frac{-e^{z_1}}{r^2}\cdot e^{z_1}
            = \frac{\partial L}{\partial y_1}\cdot\frac{-e^{2z_1}}{r^2}
        \]
        \[
            \frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial y_2}
            \cdot\frac{\partial y_2}{\partial r}
            \cdot\frac{\partial r}{\partial z_2}
            = \frac{\partial L}{\partial y_2}\cdot\frac{-e^{2z_2}}{r^2}
        \]
        \[
            \frac{\partial L}{\partial r} = \frac{\partial L}{\partial y_1}
            \cdot\frac{\partial y_1}{\partial r} + \frac{\partial L}{\partial y_2}
            \cdot\frac{\partial y_2}{\partial r}
            = \frac{\partial L}{\partial y_1}\cdot\frac{-e^{z_1}}{r^2} +
            \frac{\partial L}{\partial y_2}\cdot\frac{-e^{z_2}}{r^2}
        \]
        (ii)\,For the general d-dimentional case, we have:
        \[
            \frac{\partial L}{\partial z_i} = \frac{\partial L}{\partial y_i}
            \cdot\frac{\partial y_i}{\partial r}
            \cdot\frac{\partial r}{\partial z_i}
            = \frac{\partial L}{\partial y_i}\cdot\frac{-e^{2z_i}}{r^2}
        \]
        \[
            \frac{\partial L}{\partial r} = \sum_{i=1}^{d}\frac{\partial L}{\partial y_i}
            \cdot\frac{\partial y_i}{\partial r}
            = \sum_{i=1}^{d}\frac{\partial L}{\partial y_i}\cdot\frac{-e^{z_i}}{r^2}
        \]
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Optimization}
    \subsection*{(a)}
    \begin{homeworkSubsection}
        i.\.RMSProp\; $\beta_1=\rho^3(1-\rho), \beta_2=\rho^2(1-\rho), \beta_3=\rho(1-\rho), \beta_4=1-\rho$

        ii.\.AdaGrad\;$\beta_i=1,\forall i$
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        We modified the term into $x_{k+1} = x_k - \alpha_k(v_k + \epsilon)$
        where $v_k = \beta v_{k-1} + (1-\beta)\nabla_f(x_k)$.
        Momentum can speed up gradient descent process. 
        It can also prevent the gradient from oscillating or overshooting.
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{Convolutional Neural Networks}
    \subsection*{(a)}
    \begin{homeworkSubsection}
    \begin{table}[htbp]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
            Layer & Feature map dimensions & Number of weights & Number of biases \\
            \hline
            INPUT & $128\times128\times3$ & 0 & 0 \\
            \hline
            CONV-9-32 & $120\times120\times96$ & $9\times9\times3\times32$ & 32 \\
            \hline
            POOL-2 & $60\times60\times96$ & 0 & 0 \\
            \hline
            CONV-5-64 & $56\times56\times6144$ & $5\times5\times96\times64$ & 64 \\
            \hline
            POOL-2 & $28\times28\times6144$ & 0 & 0 \\
            \hline
            CONV-5-64 & $24\times24\times393216$ & $5\times5\times6144\times64$ & 64 \\
            \hline
            POOL-2 & $12\times12\times393216$ & 0 & 0 \\
            \hline
            FC-9 & 9 & $12\times12\times393216\times9$ & 1 \\
            \hline
        \end{tabular}
    \end{table}
    \end{homeworkSubsection}
    \subsection*{(b)}
    \begin{homeworkSubsection}
        \[
            y = \begin{pmatrix}
                x_1w_1 + x_2w_2\\
                x_2w_1 + x_3w_2\\
                x_3w_1 + x_4w_2\\
            \end{pmatrix}
        \]
        so we have $\nabla_wy_1= \left[x_1, x_2\right]^\top$, 
        $\nabla_wy_2= \left[x_2, x_3\right]^\top$,
        $\nabla_wy_3= \left[x_3, x_4\right]^\top$.
    \end{homeworkSubsection}
    \subsection*{(c)}
    \begin{homeworkSubsection}
        Overlapped groups allocates their gradients to the same max elements in the group
        which is explained \href{https://ai.stackexchange.com/questions/17107/how-can-we-compute-the-gradient-of-max-pooling-with-overlapping-regions}{here}.
    \end{homeworkSubsection}
    \subsection*{(d)}
    \begin{homeworkSubsection}
        In a fully connected network with one hidden layer,
        increasing in the hidden layer size will increase variance of the output.
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}{RNNs}
    See Assignment 11
\end{homeworkProblem}


\end{document}