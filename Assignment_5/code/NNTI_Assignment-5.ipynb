{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4 SVM (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=500, noise=0.05, random_state=42)\n",
    "df = pd.DataFrame(dict(x1=X[:, 0], x2=X[:, 1], y=y))\n",
    "\n",
    "#map classes to -1 and 1 for simplification\n",
    "df[\"y\"] = df[\"y\"].apply(lambda x: x if x else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {-1: \"red\", 1: \"blue\"}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby(\"y\")\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind=\"scatter\", x=\"x1\", y=\"x2\", label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[[\"x1\", \"x2\"]]\n",
    "y = df[\"y\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.7, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a powerful and versatile class of supervised machine learning algorithms used for classification and regression tasks. At their core, SVMs aim to find a hyperplane that best separates different classes of data. The beauty of SVMs lies in their ability to handle both linear and non-linear data using kernel functions.\n",
    "\n",
    "In a two-dimensional space, an SVM finds a line (and in higher dimensions, a hyperplane) that best separates the classes. This line is determined by the support vectors, which are the data points nearest to the hyperplane.\n",
    "The equation of the hyperplane can be represented as:\n",
    "The equation of the hyperplane can be represented as:\n",
    "$$ w \\cdot x + b = 0 $$\n",
    "where \\( w \\) is the weight vector and \\( b \\) is the bias.\n",
    "\n",
    "\n",
    "The regularized hinge loss, which is used for training Support Vector Machines with L2 regularization, is given by:\n",
    "   $$ L(w,b) = \\frac{\\lambda}{2} ||w||^2 + \\sum_{i} \\max(0, 1 - y_i(w \\cdot x_i + b)) $$\n",
    "\n",
    "where:\n",
    "- \\( \\lambda \\) is the regularization parameter.\n",
    "- \\( w \\) is the weight vector.\n",
    "- \\( b \\) is the bias term.\n",
    "- \\( x_i \\) is the feature vector of the \\( i^{th} \\) data point.\n",
    "- \\( y_i \\) is the actual label of the \\( i^{th} \\) data point, with \\( y_i \\in \\{ -1, 1 \\} \\).\n",
    "\n",
    "\n",
    "For non-linearly separable data, SVMs use the kernel trick. A kernel function transforms the data into a higher-dimensional space where it becomes linearly separable. Common kernels include the linear, polynomial, and radial basis function (RBF) kernels.\n",
    "\n",
    "### The Polynomial Kernel\n",
    "The polynomial kernel function, commonly used with SVMs, is defined as:\n",
    "\n",
    "$$ K(x_1, x_2) = (x_1^T x_2 + c)^d $$\n",
    "\n",
    "where:\n",
    "- \\( x_1 \\) and \\( x_2 \\) are two input vectors.\n",
    "- \\( x_1^T x_2 \\) represents the dot product of \\( x_1 \\) and \\( x_2 \\).\n",
    "- \\( c \\) is a constant term that allows for tuning of the kernel's behavior.\n",
    "- \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "where **c** is a constant and **x1** and **x2**  are vectors in the original space.\n",
    "\n",
    "The parameter **c** can be used to control the trade-off between the fit of the training data and the size of the margin. A large **c** value will give a low training error but may result in overfitting. A small **c** value will give a high training error but may result in underfitting. The degree **d** of the polynomial can be used to control the complexity of the model. A high degree d will result in a more complex model that may overfit the data, while a low degree **d** will result in a simpler model that may underfit the data.\n",
    "\n",
    "When a dataset is given containing features x1 and x2, the equation can be transformed as:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1^2 & x_1 x_2 \\\\\n",
    "x_1 x_2 & x_2^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The important terms we need to note are **x1**, **x2**, **x1^2**, **x2^2** , and **x1 * x2**. When finding these new terms, the non-linear dataset is converted to another dimension that has features **x1^2**, **x2^2** , and **x1 * x2**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When employing the polynomial kernel, it is crucial to ensure that you apply this kernel transformation to both the training data during the 'fit' function and the test data during the 'predict' function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, \n",
    "                C=1.0, \n",
    "                learning_rate=0.001, \n",
    "                lambda_param=1e-2, \n",
    "                kernel=None):\n",
    "        # C = error term\n",
    "        self.C = C\n",
    "        self.lambda_param = lambda_param\n",
    "        self.lr = learning_rate\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def polynomial_transform(self, X, Y=None):\n",
    "        # implement the polynomial transformation based on the equation given above\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, dw, db):\n",
    "        #update w and b based on the gradients\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def calculate_gradients(self, x, y, loss):\n",
    "        # calculate gradients\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def hinge_loss(self, output, y):\n",
    "        # implement the hinge loss based on the equation given above\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def init_weights(self, X):\n",
    "        #initlaize the wights and biases to 0\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000):\n",
    "        #Implement gradient descent\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM class with a linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an SVM with linear kernel and calulate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM class with a polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an SVM with polynomial kernel and calulate the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the transformed input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_transformed(df):\n",
    "    df[\"x1^2\"] = df[\"x1\"] ** 2\n",
    "    df[\"x2^2\"] = df[\"x2\"] ** 2\n",
    "    df[\"x1*x2\"] = df[\"x1\"] * df[\"x2\"]\n",
    "\n",
    "    fig = px.scatter_3d(df, x=\"x1^2\", y=\"x2^2\", z=\"x1*x2\", color=\"y\")\n",
    "    fig.show()\n",
    "\n",
    "plot_transformed(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SVM on real dataset (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the Support Vector Machine (SVM) with a polynomial kernel to tackle the task of classifying Flower Species. Given that this problem involves multiple classes and the SVMyou implemented is a binary classifier, employ the [one-vs-all](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/one-vs-all#:~:text=all%20provides%20a%20way%20to,classifier%20for%20each%20possible%20outcome.) classification strategy. This approach will enable you to train a separate classifier for each possible outcome, effectively addressing the multi-class nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width          species\n",
       "112           6.8          3.0           5.5          2.1   Iris-virginica\n",
       "88            5.6          3.0           4.1          1.3  Iris-versicolor\n",
       "43            5.0          3.5           1.6          0.6      Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./IRIS.csv\")\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
