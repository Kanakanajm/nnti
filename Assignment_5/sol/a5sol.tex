\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{5}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: Camilo Mart√≠nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{Gradient Descent}
    \subsection*{a)}
    First we calculate the partial derivatives:
    \begin{align*}
        \frac{\partial f}{\partial x_1} &= -2(a - x_1) - 4bx_1(x_2-x_1^2)^2\\
        \frac{\partial f}{\partial x_2} &= 2b(x_2-x_1^2)\\
    \end{align*}
    For each time step t, we have $x_t = x_{t-1} - \epsilon \nabla f(x_{t-1})$.\\
    After three iterations we have the result:
    \begin{align*}
        x_0 &= \begin{bmatrix}
            0.9\\
            1.12\\
        \end{bmatrix}\qquad
        \nabla f(x_0) &&= \begin{bmatrix}
            -111.8\\
            62\\
        \end{bmatrix}\qquad
        f(x_0) &&&= 9.62\\
        x_1 &= \begin{bmatrix}
            0.91\\
            1.11\\
        \end{bmatrix}\qquad
        \nabla f(x_1) &&= \begin{bmatrix}
            -103.52\\
            56.71\\
        \end{bmatrix}\qquad
        f(x_1) &&&= 8.05\\
        x_2 &= \begin{bmatrix}
            0.92\\
            1.11\\
        \end{bmatrix}\qquad
        \nabla f(x_2) &&= \begin{bmatrix}
            -95.59\\
            51.78\\
        \end{bmatrix}\qquad
        f(x_2) &&&= 6.71\\
        x_3 &= \begin{bmatrix}
            0.93\\
            1.10\\
        \end{bmatrix}\qquad
        \nabla f(x_3) &&= \begin{bmatrix}
            -88.04\\
            47.2\\
        \end{bmatrix}\qquad
        f(x_3) &&&= 5.58\\
    \end{align*}
    The value of $f$ decreases after the iterations.\\
    If $\epsilon_1$ is used, the changes in $x$ are very small at each iteration and $f$ will not degree that much after the iterations.\\
    If $\epsilon_2$ is used, $f$ will increase and be very large.
    \subsection*{b)}
    We can find the global minimum by setting the gradient to zero i.e. $\nabla f(x) = 0$ and solve for $x$, we get
    \begin{equation*}
        \begin{cases}
            -2(a - x_1) - 4bx_1(x_2-x_1^2)^2 &= 0\\
            2b(x_2-x_1^2) &= 0
        \end{cases}
    \end{equation*}
    After simplification we get
    \begin{equation*}
        \begin{cases}
            x_1 &= a = 1\\
            x_2 &= a^2 = 1
        \end{cases}
    \end{equation*}
    Using an iterative method, we found an $\hat{x} = \begin{bmatrix}
        0.989\\
        0.978
    \end{bmatrix} $ with learning rate of 0.002 with a gradient threshold (stop when gradient's L2 norm smaller than 0.01) after 1251 iterations.
    (See a51a.py)
\end{homeworkProblem} 
\begin{homeworkProblem}{Weight Space Symmetry}
    \subsection*{a)}
    There are $M!$ to arrange to order of the neurons and the $2$ comes from the two sign so the numbers of equavilant models is:
    $$2M!$$
    \subsection*{b)}
    For each layer there is $M_i$ possible equavilant layers and the number of combinations of the layers is given by their product.
    $$2\prod_{i=1}^{n} M_i!$$
\end{homeworkProblem}
\begin{homeworkProblem}{SVM and Kernels}
    \subsection*{a)}
    \begin{align*}
        \intertext{$L$ can be rewritten as}\\
        L(w, b, \alpha) &= \frac{1}{2}w^Tw-\sum_{i=1}^{N}\alpha_iy_i(w^T\cdot x_i) - \sum_{i=1}^{N}\alpha_i y_i b + \sum_{i=1}^{N}\alpha_i\\
        \intertext{Since $w^T\cdot x_i$ is a scalar, $w^T\cdot x_i = x_i^T \cdot w$, we get}\\
        L(w, b, \alpha) &= \frac{1}{2}w^Tw-\sum_{i=1}^{N}\alpha_iy_ix_i^T\cdot w - \sum_{i=1}^{N}\alpha_i y_i b + \sum_{i=1}^{N}\alpha_i\\
        \intertext{Now we calculate the partial derivatives of $L$ w.r.t $w$ and $b$}\\
        \intertext{For $\frac{\partial L}{\partial w}$ we get:}
        \frac{\partial L}{\partial w} &= \frac{1}{2} \cdot 2w - \sum_{i=1}^{N}\alpha_iy_ix_i\\
        \frac{\partial L}{\partial w} &= w - \sum_{i=1}^{N}\alpha_iy_ix_i\\
        \intertext{Now setting $\frac{\partial L}{\partial w} = 0$, we get}\\
        w &= \sum_{i=1}^{N}\alpha_iy_ix_i\\
        \intertext{For $\frac{\partial L}{\partial b}$ we get:}\\
        \frac{\partial L}{\partial b} &= -\sum_{i=1}^{N}\alpha_iy_i\\
        \intertext{Similarly, setting $\frac{\partial L}{\partial b} = 0$, we get}\\
        0 &= \sum_{i=1}^{N}\alpha_iy_i\\
        \intertext{Now we plug the result back to the equation of $L$}
        L(w, b, \alpha) &= \frac{1}{2}w^T\sum_{i=1}^{N}\alpha_iy_ix_i - \sum_{i=1}^{N}\alpha_iy_iw^Tx_i + \sum_{i=1}^{N}\alpha_i\\
        \intertext{We know that for vector dot product we have $a^T\sum_{i=1}^{n}b_i = \sum_{i=1}^{n}a^Tb_i$ so we can group the frist two terms together into}\\
        L(w, b, \alpha) &= -\frac{1}{2}w^T\sum_{i=1}^{N}\alpha_iy_ix_i + \sum_{i=1}^{N}\alpha_i\\
        L(w, b, \alpha) &= -\frac{1}{2}(\sum_{j=1}^{N}\alpha_jy_jx_j)^T\sum_{i=1}^{N}\alpha_iy_ix_i + \sum_{i=1}^{N}\alpha_i\\
        L(w, b, \alpha) &= -\frac{1}{2}\sum_{j=1}^{N}\alpha_jy_jx_j^T\sum_{i=1}^{N}\alpha_iy_ix_i + \sum_{i=1}^{N}\alpha_i\\
        L(w, b, \alpha) &= -\frac{1}{2}\sum_{j=1}^{N}\alpha_jy_jx_j^T\sum_{i=1}^{N}\alpha_iy_ix_i + \sum_{i=1}^{N}\alpha_i\\
        \intertext{which is equavilant to}
        L(w, b, \alpha) &= -\frac{1}{2}\sum_{j=1}^{N}\sum_{i=1}^{N}\alpha_jy_j\alpha_iy_ix_j^Tx_i + \sum_{i=1}^{N}\alpha_i\\
    \end{align*}
    \subsection*{b)}
    The lagrange multiplier $\alpha_i$ make sure the constraint is active so that they must be non-negative
    \subsection*{d)}
    $$
    L(w, b, \alpha) = -\frac{1}{2}\sum_{j=1}^{N}\sum_{i=1}^{N}\alpha_jy_j\alpha_iy_ik(x_j,x_i) + \sum_{i=1}^{N}\alpha_i\\
    $$
\end{homeworkProblem}
\end{document}