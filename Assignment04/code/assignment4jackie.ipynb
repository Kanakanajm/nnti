{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d299a6-0e8e-4a68-948f-20be6e940377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from typing import List, Tuple  # For typehints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1a29d-a1b1-41a1-9980-8ae4fcb1a501",
   "metadata": {},
   "source": [
    "# Exercise 4.3 (4 pts)\n",
    "The goal of this notebook is to gain further understanding how bias and variance are influenced by various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda34661-0d57-438e-a777-1f6698922e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sin(rng: np.random.Generator, n: int = 10):\n",
    "    \"\"\"\n",
    "    Returns a random sample of size n with y = sin(x) + eps\n",
    "    \"\"\"\n",
    "    X = np.sort(rng.uniform(0, 7, size=(n, 1)), axis=0)\n",
    "    y = np.sin(X) + rng.normal(size=(n, 1), scale=0.5)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0694e-ac2f-4909-a5a0-a4d798f4efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sample_sin(np.random.default_rng(0))\n",
    "X_test = np.linspace(0, 7).reshape(-1, 1)\n",
    "y_test = np.sin(X_test).reshape(-1, 1)\n",
    "plt.scatter(X, y, marker=\".\")\n",
    "plt.plot(X_test, y_test, label=\"$f(x)$\", linestyle=\"dashed\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b8550-d9ac-4bf7-8f06-2b4fc3ffadb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4.3.1 (0.5pts)\n",
    "Once again implement the fit methods for the `linear_regression` and `ridge_regression` models by hand.\n",
    "Make sure to add a column of ones to the data matrix to model the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc7234-66a6-4993-9112-9764c2ade619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "\n",
    "    def __init__(self, include_bias=True):\n",
    "        self.include_bias = include_bias\n",
    "\n",
    "    def get_design_matrix(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adds a column of ones to the input matrix to model the bias\n",
    "        \"\"\"\n",
    "        if self.include_bias:\n",
    "            dummy = np.ones((X.shape[0], 1))\n",
    "            return np.hstack([X, dummy])\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fits the model using the input features X and labels y\n",
    "        :param X: (N, D) numpy array, each row is one observation\n",
    "        :param y: (N, 1) numpy array, one target for each observation\n",
    "        \"\"\"\n",
    "        # Set self.w\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        phi = self.get_design_matrix(X_test)\n",
    "        return phi @ self.w\n",
    "\n",
    "\n",
    "class linear_regression(Estimator):\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        # self.w = ...\n",
    "        return self\n",
    "\n",
    "\n",
    "class ridge_regression(Estimator):\n",
    "    def __init__(self, lambda_: float, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        # self.w = ...\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7af54d-447e-41ea-8e64-912de9105712",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4.3.2 Polynomial features & model complexity (0.5pts)\n",
    "\n",
    "In this exercise, you will explore the impact of polynomial features on linear regression and ridge regression models and how the role of regularization controls the model complexity. Polynomial features are a form of basis expansion where instead of using just $X$ we add powers of $X$, e.g. $X^{d}, X^{d-1}, \\dots X^2, X^1$,\n",
    "resulting in a linear regression model with $d$ parameters:\n",
    "\n",
    "$$\n",
    "    \\hat{f}(x) = w_0 + \\sum_{i=1}^{d-1} w_i \\cdot x^{i}\n",
    "$$\n",
    "\n",
    "This allows the model to fit non-linear relationships.\n",
    "\n",
    "This technique allows linear models to capture more complex patterns in the data.\n",
    "\n",
    "Complete the following function by fitting a linear regression and a ridge regression model on polynomial features for each degree.\n",
    "To obtain the polynomial features you can use sklearns `PolynomialFeatures`. Note that either on `PolynomialFeatures` or the model class `include_bias` should be set to false.\n",
    "\n",
    "For each degree, also plot the fit of both the linear and ridge regression models, as well as the true model given by $y = sin(x)$, to visually compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ea9b2-7ed7-45c2-91eb-ad31836f2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_degree(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, d: int = 7) -> None:\n",
    "    \"\"\"\n",
    "    Plots the fit for linear and ridge regression with polynomial basis of degrees 1 to d\n",
    "    E.g. for degree 2 we plot the linear/ridge regression model w_1 * X^2 + w_2 * X + w_0\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(d-1, 2, sharex=\"all\", sharey=\"all\")\n",
    "    fig.set_size_inches(10, 7)\n",
    "    axes[0,0].set_title(\"Linear regression\")\n",
    "    axes[0,1].set_title(\"Ridge regression\")\n",
    "\n",
    "    # Plot linear regression fit in first row, ridge regression in second row\n",
    "    # Don't forget to transform the features and to plot the true function\n",
    "    \n",
    "    # Your code here ...\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa1fa5-410f-4a14-af6a-a305be3fa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_degree(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c73f60-a488-4cb3-bf95-d45b1ce65a1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4.3.3 Cross validation (0.5 + 0.5)\n",
    "In the lecture you have seen how cross validation can be used to find the \"best\" hyperparameters of a model.\n",
    "In this exercise you will implement leave-one-out cross validation for the ridge regression model.\n",
    "\n",
    "Leave-one-out cross validation (LOOCV) is a special case of $k$-fold cross validation with $k=N$, i.e. the number of datapoints.\n",
    "In LOOCV, we hold out each datapoint once and fit the model on the $N-1$ left over datapoints. The test error is then calculated on the held out datapoint.\n",
    "\n",
    "Implement `loocv` and complete the `select_best_params` function to find the best degree, $\\lambda$ combination in terms of generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39080b1a-b26d-46c3-86a0-d7083b35296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv(X: np.ndarray, y: np.ndarray, degree=1, lambda_=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the generalization error by averaging the squared error of N ridge regression models each fitted on N-1 datapoints.\n",
    "    For each model the error is calculated on the left out datapoint.\n",
    "    \"\"\"\n",
    "    # Your code here ...\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cf059-e3cf-40e1-a78e-ba311588bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_params(X, y) -> Tuple[ridge_regression, PolynomialFeatures]:\n",
    "    \"\"\"\n",
    "    Selects the best degree/lambda combination using LOOCV.\n",
    "\n",
    "    :returns: A ridge regression model with the best regulariztation strength lambda and a PolyinomialFeatures instance\n",
    "        with the best degree.\n",
    "    \"\"\"\n",
    "    degrees = list(range(1, 7))\n",
    "    lambdas = [0.01, 0.1, 0.5, 1, 2]\n",
    "\n",
    "    # Find the best degree, lambda combination by testing all possible configurations and\n",
    "    # selecting the one with the lowest loocv error\n",
    "\n",
    "    # Your code here ...\n",
    "    \n",
    "    return model, poly_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adad700-5050-445f-bb1e-2d44f6ab43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, feature_transformer = select_best_params(X, y)\n",
    "model.fit(feature_transformer.fit_transform(X), y)\n",
    "plt.plot(X_test, model.predict(feature_transformer.fit_transform(X_test)))\n",
    "plt.plot(X_test, y_test)\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c5426-f086-4d10-b41e-2f8d165e3803",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4.3.4 Model variance (0.5pts)\n",
    "In this part we will explore how much a model changes based on the dataset, i.e. the variance of the model.\n",
    "\n",
    "You are given a sample function that returns a random dataset of size $n$ with $y = 3*x + eps$.\n",
    "\n",
    "Explore the variance of simple linear regression and ridge regression by sampling 5 datasets and plotting the model fit, as well as the ground truth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff469421-03cd-404f-afd8-f64ddc436875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rng, n: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns a random sample of size n with y = 3*x + eps\n",
    "    \"\"\"\n",
    "    X = rng.uniform(0, 7, size=(n,1))\n",
    "    y = 3*X + rng.normal(size=(n, 1), scale=10)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e24e42-07be-4273-8b00-9703d5a4f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_effect(lambda_: float) -> None:\n",
    "    \"\"\"\n",
    "    Plots the model variance of linear and ridge regression models\n",
    "\n",
    "    :param lambda_: Regularization strength\n",
    "    \"\"\"\n",
    "    X_test = np.linspace(0, 7).reshape(-1, 1)\n",
    "    y_test = 3*X_test\n",
    "    rng = np.random.default_rng(0)\n",
    "    # One ax for each dataset\n",
    "    fig, axes = plt.subplots(1, 5, sharex=\"all\", sharey=\"all\")\n",
    "    fig.set_size_inches(15, 5)\n",
    "\n",
    "    # Sample 5 datasets, fit linear and ridge regression model for each and plot the fit versus the true function\n",
    "    # It is enlighting to also plot the dataset.\n",
    "    # Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982685d1-6c8e-4318-b1b1-250e5cecbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are encouraged to play around with lambda to see how this influences the variability of the ridge regression model\n",
    "variance_effect(lambda_ = 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf5b25-7e7b-4e6f-acf8-859022cb9ae4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 4.3.5 Dataset size influence on bias and variance (1 + 0.5)\n",
    "\n",
    "Assuming all datasets of size $n$ are equally likely, we can approximate the expected weights $\\mathbb{E}[(\\hat{w_0}, \\hat{w_1})^T]$ by **averaging** the weights of a large number of models.\n",
    "Furthermore, as long as the true process $f(x) = w_1 \\cdot x + w_0$ is linear, the least squares estimate is unbiased, that is $\\mathbb{E}_{D_n \\sim P(X)} [(\\hat{w}_0, \\hat{w_1})^T] = (w_0, w_1)^T$. Use those facts to calculate bias and variance estimates of a simple model with $f(x) = 3x$.\n",
    "As a reminder the variance of the model is defined as $Var(\\mathbf{\\hat{w}}) = \\mathbb{E}[(\\mathbf{\\hat{w}} - \\mathbb{E}[\\mathbf{\\hat{w}}])^2]$\n",
    "\n",
    "Also complete the `plot_bias_variance` function to showcase the effect of the dataset on bias and variance visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad370ad-a104-41d6-974e-ae51e5bc3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance(n: int, rng: np.random.Generator, n_samples=10_000) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates bias and variance of the linea regression parameter for datasets of size n\n",
    "    for the simple case where f(x) = 3*x\n",
    "\n",
    "    Bias is defined as w - E[w] and variance is given by E[(w - E[w])**2]\n",
    "    \n",
    "    :param n: Size of the datatsets\n",
    "    :param rng: numpy random number generator for reproducability\n",
    "    :param n_samples: Number of samples to estimate the expected value. Should be quite high for an accurate representation\n",
    "    :return: Tuple with bias and variance of the model parameters\n",
    "    \"\"\"\n",
    "    # To estimate the expectations draw n_samples datasets of size n and approximate by averaging\n",
    "    # Your code here ...\n",
    "    \n",
    "    return bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf00a4f-62a0-4e45-b0ad-9bc30d9e5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance(sizes: List[int]):\n",
    "    \"\"\"\n",
    "    Plots the bias and variance for different dataset sizes\n",
    "\n",
    "    :param sizes: List of dataset sizes\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0)\n",
    "    bias_results = []\n",
    "    variance_results = []\n",
    "    # Calculate bias and variances for all sizes\n",
    "    # Your code here ...\n",
    "    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=\"all\")\n",
    "    ticks = range(len(sizes))\n",
    "    # Plot variance\n",
    "    ax1.plot(ticks, variance_results)\n",
    "    ax1.set_xticks(ticks, labels=sizes)\n",
    "    ax1.set_xlabel(\"Dataset size\")\n",
    "    ax1.set_ylabel(\"Variance\")\n",
    "\n",
    "    # Plot bias\n",
    "    ax2.plot(ticks, bias_results)\n",
    "    ax2.set_xticks(ticks, labels=sizes)\n",
    "    ax2.set_xlabel(\"Dataset size\")\n",
    "    ax2.set_ylabel(\"Bias\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c523b8e-6e55-49bd-a0c2-3b38107bbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance([5, 6, 7, 8, 9, 10, 20, 30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
