\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{changepage}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{6}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: \\ Camilo Mart√≠nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}
%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Activation Functions}

    \subsection*{(a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}

    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{- Introduction to convexity}

    \subsection*{(a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}

    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{- General Questions}

    \subsection*{a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    They introduce non-linearity, which in turns helps learn more complex functions. Otherwise, our Neural Networks would just be multiplication of weight matrices, which obviously can be represented by a single weight matrix. 
    \end{adjustwidth}

    \subsection*{b)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    Faster to compute and numerically stable, since it does not involve complex mathematical operations that may under- or overflow, such as exponentials in the sigmoid function or a geometrical one like tanh.
    \end{adjustwidth}

    \subsection*{c)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    
    \end{adjustwidth}

    \subsection*{d)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    Because it involves calculating the inverse of a matrix, which is an extremely costly operation with a time complexity of $O(n^3)$.
    \end{adjustwidth}

    \subsection*{e)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    No, in general any method that numerically attempts to find a minimum only guarantees a local minimum. That is, it may very well exist another minimum that could be found with a different initialization of parameters. A true minimum could theoretically be found, if we knew the mathematical formula of the loss function and that is not feasible with Neural Networks.
    \end{adjustwidth}

    \subsection*{f)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    
    \end{adjustwidth}

    \subsection*{g)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    
    \end{adjustwidth}

    \subsection*{h)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    
    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{- BatchNorm}
    The paper centers around a technique called Batch Normalization, which consists of introducing additional network layers that control the first two moments (mean and variance) of the distributions of layer inputs. The authors aim to show that the practical success of using Batch Normalization in a Neural Network does not come from reducing "Internal Covariate Shift" (which refers to the continual change in the distribution of layer inputs caused by updates to the preceding layers and is conjectured that it negatively impacts training). Instead, the authors argue that its benefits, mainly faster training and convergence, come from the fact that Batch Normalization smoothens the optimization landscape, effectively reparametrizing the underlying optimization problem to make it more stable and smooth. This implies that the gradients used in
    training are more predictive and well-behaved, which enables faster and more effective optimization. In simpler words, their findings showed that the primary benefit of BatchNorm lies in facilitating smoother gradients rather than stabilizing layer inputs.

\end{homeworkProblem}

\begin{homeworkProblem}{- Forward Pass for a Fully-Connected Neural Network}

Given the weights of the simple Feed Forward Neural Network (FFNN) with one hidden layer, defined in the problem:

\[
    W^{(1)} = 
    \begin{bmatrix}
    -0.2 & 0.9 & 0.4 \\
    -0.1 & 0.3 & 0.4 \\
    0.2 & 0.5 & -0.7 \\
    0.2 & -0.5 & 0.5
    \end{bmatrix}
    \quad
    W^{(2)} = 
    \begin{bmatrix}
    0.6 & -0.2 \\
    -0.1 & 0.8 \\
    -0.5 & -0.3
    \end{bmatrix}
\]

where $W^{(1)}$ is the weight matrix that acts upon the $1^{\text{st}}$ layer (input layer) and $W^{(2)}$, the weight matrix that acts upon the activations of the $2^{\text{nd}}$ layer (hidden layer). \\

The forward pass in this Network would look like this:
\[
    \bm{h} = 
    \begin{bmatrix}
    h_1 \\
    h_2 \\
    h_3 
    \end{bmatrix}
    = \mathtt{ReLU}\left(\left[W^{(1)}\right]^\top\bm{x}\right) = 
    \mathtt{ReLU}\left(
        \begin{bmatrix}
        -0.2 & -0.1 & 0.2 & 0.2 \\
        0.9 & 0.3 & 0.5 & -0.5 \\
        0.4 & 0.4 & -0.7 & 0.5
        \end{bmatrix}
        \begin{bmatrix}
        3 \\
        1 \\
        -1 \\
        2 
        \end{bmatrix}
    \right) =
    \mathtt{ReLU}\left(
        \begin{bmatrix}
        -0.5 \\
        1.5 \\
        3.3
        \end{bmatrix}
    \right) =
    \begin{bmatrix}
    0 \\
    1.5 \\
    3.3
    \end{bmatrix}
\]

\[
    \bm{o} = 
    \begin{bmatrix}
    o_1 \\
    o_2 
    \end{bmatrix}
    = \mathtt{Softmax}\left(\left[W^{(2)}\right]^\top\bm{h}\right) = 
    \mathtt{Softmax}\left(
        \begin{bmatrix}
        0.6 & -0.1 & -0.5 \\
        -0.2 & 0.8 & -0.3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        1.5 \\
        3.3
        \end{bmatrix}
    \right) =
    \mathtt{Softmax}\left(
        \begin{bmatrix}
        -2.1 \\
        0.31
        \end{bmatrix}
    \right) =
    \begin{bmatrix}
        0.08241332 \\ 
        0.91758668
    \end{bmatrix}
\]

\end{homeworkProblem}

\begin{homeworkProblem}{- Clustering Neural Networks}
See attached files.

\end{homeworkProblem}

\end{document}