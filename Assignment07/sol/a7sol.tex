\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{changepage}

\usetikzlibrary{automata,positioning}

\usepackage{biblatex} 
\addbibresource{bibliography.bib} % Import the bibliography file

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{7}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: \\ Camilo MartÃ­nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Maximum Likelihood Estimation and Cross-Entropy}

    \subsection*{(a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}

    Given a set of $m$ i.i.d. samples \( X = \{x^{(1)}, ..., x^{(m)}\} \) drawn a data-generating distribution \( p_{\text{data}}(x) \), the likelihood of the parameters \( \theta \) given the samples is defined as:

    \[ L(\theta | X) = \prod_{i=1}^{m} p_{\text{model}}(x^{(i)}; \theta) \]
    
    Or, as a $\log{}$-likelihood which is easier to work with:
    
    \[ \ell(\theta) = \log L(\theta | X) = \sum_{i=1}^{m} \log p_{\text{model}}(x^{(i)}; \theta) \]

    Then, the maximum likelihood estimator for \( \theta \) maximizes this likelihood:
    
    \begin{equation}\label{7.1.1} 
        \hat{\theta} = \underset{\theta}{\rm argmax}\, \ell(\theta) = \underset{\theta}{\rm argmax}\, \sum_{i=1}^{m} \log p_{\text{model}}
    \end{equation}
    \end{adjustwidth}
    
    \subsection*{(b)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    The empirical distribution \( \hat{p}_{\text{data}}(x) \) is a non-parametric estimate of the distribution that is calculated using the amount of times a sample appears in the dataset. It is an initial estimate that uses the most basic definition of probability, that is, $\text{counts}/\text{total number of samples}$. The data-generating distribution is the true underlying probability distribution from which the samples are drawn. It is unknown and it is the one we are trying to learn with \( p_{\text{model}}(x; \theta) \).
    \end{adjustwidth}

    \subsection*{(c)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    We can multiply \eqref{7.1.1} by $1/m$ because the $\rm argmax$ does not change after rescaling the function we want to maximize. After doing this, effectively obtaining a \textit{mean}, we obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution \(\hat{p}_{\text{data}}\) defined by the training data \cite{Goodfellow-et-al-2016}:
        
    \begin{equation}\label{7.1.2}
        \hat{\theta} = \underset{\theta}{\rm argmax}\, \frac{1}{m} \sum_{i=1}^{m} \log p_{\text{model}} = \underset{\theta}{\rm argmax}\, \mathbb{E}_{x \sim \hat{p}_{\text{data}}} \log p_{\text{model}}(x; \theta)
    \end{equation}
        
    \end{adjustwidth}

    \subsection*{(d)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    The KL divergence is given by

    \begin{equation}\label{7.1.3}
        D_{KL}(\hat{p}_{\text{data}} \parallel p_{\text{model}}) = \mathbb{E}_{x \sim \hat{p}_{\text{data}}} \left[\log{\frac{\hat{p}_{\text{data}}(x)}{p_{\text{model}}(x)}}\right] = \mathbb{E}_{x \sim \hat{p}_{\text{data}}} [\log \hat{p}_{\text{data}} (x) - \log p_{\text{model}}(x)]
    \end{equation}

    The term on the left of \eqref{7.1.3} is a function only of the data-generating process, not the model. This means when we train the model to minimize the KL divergence, we need only minimize the following \cite{Goodfellow-et-al-2016}:
    \[ -\mathbb{E}_{x \sim \hat{p}_{\text{data}}} [\log p_{\text{model}}(x)] \]
    
    which is of course the same as maximizing its positive counterpart:
    \[ \mathbb{E}_{x \sim \hat{p}_{\text{data}}} [\log p_{\text{model}}(x)] \]
    
    which is equal to the same maximization process in \eqref{7.1.2}.
    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{- Backpropagation}
    Given the weights

    \[
        W_{\text{hidden}} = 
        \begin{bmatrix}
        0.15 & -0.25 & 0.05 \\
        0.2 & 0.1 & -0.15 
        \end{bmatrix}
        \quad
        W_{\text{out}} = 
        \begin{bmatrix}
        0.2 & 0.5 \\
        -0.35 & 0.15 \\
        0.15 & -0.2
        \end{bmatrix}
    \]

    \begin{itemize}
        \item Perform a Forward-propagation with the given input $\bm{x}$ and compute the loss $L(1)$.
        \[
            \bm{h} = 
            \begin{bmatrix}
            h_1 \\
            h_2 \\
            h_3 
            \end{bmatrix}
            = \underset{\alpha = 0.01}{\mathtt{Leaky ReLU}}\left(W_{\text{hidden}}^\top\bm{x}\right) = 
            \underset{\alpha = 0.01}{\mathtt{Leaky ReLU}}\left(
                \begin{bmatrix}
                    0.15 & -0.25 & 0.05 \\
                    0.2 & 0.1 & -0.15 
                    \end{bmatrix}^\top
                \begin{bmatrix}
                -1 \\
                 1
                \end{bmatrix}
            \right) =
                \begin{bmatrix}
                0.05 \\  
                0.35 \\ 
                -0.002  
                \end{bmatrix}
        \]

        \[
            \bm{\hat{o}} = 
            \begin{bmatrix}
            \hat{o}_1 \\
            \hat{o}_2 
            \end{bmatrix}
            = \mathtt{Softmax}\left(W_{\text{out}}^\top\bm{h}\right) = 
            \mathtt{Softmax}\left(
                \begin{bmatrix}
                    0.2 & 0.5 \\ 
                    -0.35 & 0.15 \\ 
                    0.15 & -0.2
                \end{bmatrix}^\top
                \begin{bmatrix}
                    0.05 \\  
                    0.35 \\ 
                    -0.002  
                \end{bmatrix}
            \right) =
            \mathtt{Softmax}\left(
                \begin{bmatrix}
                    -0.1128 \\  
                    0.0779
                \end{bmatrix}
            \right) =
                \begin{bmatrix}
                    0.45246896 \\ 
                    0.54753104
                \end{bmatrix}
        \]
        
        With a one-hot encoded true label of 
        \[
            \bm{o} = \begin{bmatrix}
                o_1 \\
                o_2
            \end{bmatrix} =
            \begin{bmatrix}
                1 \\ 
                0
            \end{bmatrix}
        \]
        
        The loss $L(1)$ would be
        \[
            L(1) = -\sum_{i=1}^2 o_i \log{\hat{o}_i} = 1 \times \log{(0.45246896)} + 0 \times \log{(0.54753104)} = 0.79303612
        \]
        
        \item Use the chain rule and write down the expressions used for back propagation.
        
        Let us first write the important equations for our Feedforward Neural Network:
        \begin{equation}\label{7.2.1}
            \bm{z^{(1)}} = W_{\text{hidden}}^\top \bm{x}
        \end{equation}
        \begin{equation}\label{7.2.2}
            \bm{a^{(1)}} = \bm{h} = \underset{\alpha = 0.01}{\mathtt{Leaky ReLU}}(\bm{z^{(1)}})
        \end{equation}
        \begin{equation}\label{7.2.3}
            \bm{z^{(2)}} = W_{\text{out}}^\top \bm{a^{(1)}}
        \end{equation}
        \begin{equation}\label{7.2.4}
            \bm{a^{(2)}} = \bm{\hat{o}} = \mathtt{Softmax}(\bm{z^{(2)}})
        \end{equation}
        \begin{equation}\label{7.2.5}
            L = -\sum_{i=1}^2 o_i \log{\hat{o}_i}     
        \end{equation}
        
        Now, let us first apply the chain-rule to obtain $\partial L / \partial \bm{z^{(2)}}$
        \begin{equation}\label{7.2.6}
            \frac{\partial L}{\partial \bm{z^{(2)}}} = \frac{\partial L}{\partial \bm{\hat{o}}}\frac{\partial \bm{\hat{o}}}{\partial \bm{z^{(2)}}}
        \end{equation}

        It is easy to see that 
        \begin{equation}\label{7.2.7}
            \frac{\partial L}{\partial \hat{o}_i} = -\frac{o_i}{\hat{o}_i}
        \end{equation}

        where $i$ is the position of the true label, since we know that the true output $\bm{o}$ is one-hot encoded (the true class labels are binary, i.e. 0 or 1), that is, the derivative becomes zero in every other $j$ position, where $i \neq j$.
        
        On the other hand, we know that the every $i$-th element of the predicted output $\bm{\hat{o}}$ is defined by 
        \[
            \hat{o}_i = \frac{e^{z^{(2)}_i}}{\sum_{k} e^{z^{(2)}_k}}
        \]
        
        With this, we have to consider two cases for the derivative $\partial \hat{o} / \partial \bm{z^{(2)}}$. When $i = j$, we are differentiating the softmax function applied to $\hat{o}_i$ with respect to its own logit (the class $i$), that means, we can apply standard derivation with the quotient rule as follows

        \[
        \frac{\partial \hat{o}_i}{\partial z^{(2)}_i} = \frac{e^{z^{(2)}_i} (\sum_{k} e^{z^{(2)}_k}) - e^{z^{(2)}_i}e^{z^{(2)}_i}}{(\sum_{k} e^{z^{(2)}_k})^2} = \hat{o}_i \left(1 - \hat{o}_i\right)
        \]

        When $i \neq j$, the derivative of the exponential $e^{z^{(2)}_i}$ for the $i$-th class with respect to the $j$-th element is 0, therefore

        \[
        \frac{\partial \hat{o}_i}{\partial z^{(2)}_j} = \frac{0 - e^{z^{(2)}_i}e^{z^{(2)}_j}}{(\sum_{k} e^{z^{(2)}_k})^2} = -\hat{o}_i \hat{o}_j
        \]

        Thus,
        \begin{equation}\label{7.2.8}
            \frac{\partial \hat{o}_i}{\partial z^{(2)}_j} = 
            \begin{cases} 
            \hat{o}_i(1 - \hat{o}_i) & \text{if } i = j \\
            -\hat{o}_i \hat{o}_j & \text{if } i \neq j
            \end{cases}
        \end{equation}

        Combining \eqref{7.2.7} and \eqref{7.2.8} into \eqref{7.2.6}, we sum over the contributions of each $\hat{o}_i$ to each $z_j$ through matrix-vector multiplication. This way each $i$-th element of the resulting vector represents the accumulated derivative of the loss with respect to each $z_i$. That is,
        \[
        \frac{\partial L}{\partial z^{(2)}_i} = \sum_{i} \frac{\partial L}{\partial \hat{o}_i} \frac{\partial \hat{o}_i}{\partial z^{(2)}_i} = \sum_{i} \left( -\frac{o_i}{\hat{o}_i} \right) \frac{\partial \hat{o}_i}{\partial z^{(2)}_i} = -\frac{o_i}{\hat{o}_i} \hat{o}_i (1 - \hat{o}_i) - \sum_{j \neq i} \frac{o_i}{\hat{o}_i} (-\hat{o}_i \hat{o}_j) = -o_i (1 - \hat{o}_i) + \hat{o}_i \sum_{j \neq i} o_j
        \]

        Again, since the true labels are one-hot encoded, the term $\sum_{j \neq i} o_j$ becomes 0 for the $i$-th class and $o_i = 1$. Thus,
        \begin{equation}\label{7.2.9}
            \frac{\partial L}{\partial z^{(2)}_i} = -o_i + o_i \hat{o}_i + \hat{o}_i (0) = \hat{o}_i - o_i \rightarrow \frac{\partial L}{\partial \bm{z^{(2)}}} = \bm{\hat{o}} - \bm{o}
        \end{equation}

        On the other hand, from \eqref{7.2.3} we know that $\partial \bm{z^{(2)}} / \partial W_{\text{out}} = \bm{a^{(1)}}$. Therefore, we can state
        \begin{equation}\label{7.2.10}
            \frac{\partial L}{\partial W_{\text{out}}} = \frac{\partial L}{\partial \bm{z^{(2)}}} \frac{\partial \bm{z^{(2)}}}{\partial W_{\text{out}}} = \bm{a^{(1)}} (\bm{\hat{o}} - \bm{o})^\top
        \end{equation}
        
        Similarly, we can use the chain rule as follows 
        \begin{equation}\label{7.2.11}
            \frac{\partial L}{\partial W_{\text{hidden}}} = \frac{\partial L}{\partial \bm{a^{(1)}}} \frac{\partial \bm{a^{(1)}}}{\partial \bm{z^{(1)}}} \frac{\partial \bm{z^{(1)}}}{\partial W_{\text{hidden}}}
        \end{equation}

        where, from the chain rule, we realize
        \begin{equation}\label{7.2.12}
            \frac{\partial L}{\partial \bm{a^{(1)}}} = \frac{\partial L}{\partial \bm{z^{(2)}}} \frac{\partial \bm{z^{(2)}}}{\partial \bm{a^{(1)}}} = W_{\text{out}} (\bm{\hat{o}} - \bm{o})
        \end{equation}
    
        From \eqref{7.2.2} we get 
        \begin{equation}\label{7.2.13}
            \frac{\partial \bm{a^{(1)}}}{\partial \bm{z^{(1)}}} = \frac{\partial \bm{h}}{\partial \bm{z^{(1)}}} = \begin{cases} 
            1 & \text{for } z_i^{(1)} > 0 \\
            0.01 & \text{for } z_i^{(1)} \leq 0
            \end{cases}
        \end{equation}

        And from \eqref{7.2.1} we get 
        \begin{equation}\label{7.2.14}
            \frac{\partial \bm{z^{(1)}}}{\partial W_{\text{hidden}}} = \bm{x}
        \end{equation}
        
        Finally, we can combine \eqref{7.2.12}, \eqref{7.2.13} and \eqref{7.2.14} to calculate the remaining gradient
        \begin{equation}\label{7.2.15}
            \frac{\partial L}{\partial W_{\text{hidden}}} = \bm{x} \, \left[W_{\text{out}} (\bm{\hat{o}} - \bm{o}) \odot \frac{\partial \bm{h}}{\partial \bm{z^{(1)}}}\right]^\top
        \end{equation}

        where $\odot$ denotes element-wise multiplication.

        \item Compute the Back-propagation and apply Gradient descent with a learning rate of 0.1 to update the weights.
        
        Using the previously defined equations, we calculate 

        \[
        \frac{\partial L}{\partial \bm{z^{(2)}}} = \bm{\hat{o}} - \bm{o} = 
        \begin{bmatrix}
        0.45246896 \\ 0.54753104    
        \end{bmatrix} - \begin{bmatrix}
        1 \\ 0    
        \end{bmatrix} = \begin{bmatrix}
        -0.54753104 \\ 0.54753104
        \end{bmatrix}
        \]
        \[
            \frac{\partial L}{\partial W_{\text{out}}} = \bm{a^{(1)}}(\bm{\hat{o}} - \bm{o})^\top =
            \begin{bmatrix}
                0.05 \\ 0.35 \\ -0.002
            \end{bmatrix} \cdot
            \begin{bmatrix}
                -0.54753104 \\ 0.54753104
            \end{bmatrix}^\top = \begin{bmatrix}
                -0.02737655 & 0.02737655 \\
                -0.19163586 & 0.19163586 \\
                0.00109506 & -0.00109506                    
            \end{bmatrix}
        \]
        \[
            \frac{\partial \bm{a^{(1)}}}{\partial \bm{z^{(1)}}} = 
            \frac{\partial \bm{h}}{\partial \bm{z^{(1)}}} = 
            \begin{bmatrix}
                1. \\
                1.o  \\
                0.01 
            \end{bmatrix}
        \]
        \[
            \frac{\partial L}{\partial W_{\text{hidden}}} = 
            \begin{bmatrix}
                -1 \\ 1 
            \end{bmatrix}\left[
            \begin{bmatrix}
                0.2 & 0.5 \\
                -0.35 & 0.15 \\
                0.15 & -0.2 
            \end{bmatrix} \cdot
            \begin{bmatrix}
                -0.54753104 \\ 0.54753104
            \end{bmatrix} \odot 
            \begin{bmatrix}
                1. \\
                1. \\
                0.01 
            \end{bmatrix}\right]^\top = 
            \begin{bmatrix}
                -0.16425931 & -0.27376552 & 0.00191636 \\
                0.16425931 & 0.27376552 & -0.00191636
            \end{bmatrix}
        \]

        With these gradients, we update the weights like so 
        \begin{align*}
            W_{\text{hidden}} &= W_{\text{hidden}} - \, \alpha \, \frac{\partial L}{\partial W_{\text{hidden}}}
            = 
            \begin{bmatrix}
                0.15 & -0.25 & 0.05 \\
                0.2 & 0.1 & -0.15 
            \end{bmatrix} - 0.1 \, \begin{bmatrix}
                -0.16425931 & -0.27376552 & 0.00191636 \\
                0.16425931 & 0.27376552 & -0.00191636
            \end{bmatrix} = \dots \\
            &= \begin{bmatrix}
                0.16642593 & -0.22262345 & 0.04980836 \\
                0.18357407 & 0.07262345 & -0.14980836
            \end{bmatrix}
        \end{align*}
        \[
            W_{\text{out}} = W_{\text{out}} - \, \alpha \, \frac{\partial L}{\partial W_{\text{out}}}
            \begin{bmatrix}
                0.2 & 0.5 \\
                -0.35 & 0.15 \\
                0.15 & -0.2
            \end{bmatrix}
            - 0.1 \, \begin{bmatrix}
                -0.02737655 & 0.02737655 \\
                -0.19163586 & 0.19163586 \\
                 0.00109506 & -0.00109506
            \end{bmatrix} = \begin{bmatrix}
                0.20273766 & 0.49726234 \\
                -0.33083641 & 0.13083641 \\
                0.14989049 & -0.19989049
            \end{bmatrix}
        \]

        \item Perform Forward-propagation again with the updated weights and recompute the loss $L(2)$. Briefly explain your findings.
            
        Now let us perform Forward-propagation again
        \begin{align*}
            \bm{h} = 
            \begin{bmatrix}
            h_1 \\
            h_2 \\
            h_3 
            \end{bmatrix}
            &= \underset{\alpha = 0.01}{\mathtt{Leaky ReLU}}\left(W_{\text{hidden}}^\top\bm{x}\right) = 
            \underset{\alpha = 0.01}{\mathtt{Leaky ReLU}}\left(
                \begin{bmatrix}
                    0.16642593 & -0.22262345 & 0.04980836 \\
                    0.18357407 & 0.07262345 & -0.14980836
                \end{bmatrix}^\top
                \begin{bmatrix}
                -1 \\
                 1
                \end{bmatrix}
            \right) = \dots \\
                &= \begin{bmatrix}
                0.01714814 \\ 0.2952469 \\ -0.00199617
                \end{bmatrix}
        \end{align*}
        \begin{align*}
            \bm{\hat{o}} = 
            \begin{bmatrix}
            \hat{o}_1 \\
            \hat{o}_2 
            \end{bmatrix}
            &= \mathtt{Softmax}\left(W_{\text{out}}^\top\bm{h}\right) = 
            \mathtt{Softmax}\left(
                \begin{bmatrix}
                    0.20273766 & 0.49726234 \\
                    -0.33083641 & 0.13083641 \\
                    0.14989049 & -0.19989049
                \end{bmatrix}^\top
                \begin{bmatrix}
                    0.01714814 \\ 0.2952469 \\ -0.00199617
                \end{bmatrix}
            \right) = \dots \\
            &=
            \begin{bmatrix}
                    0.46454554 \\ 0.53545446
            \end{bmatrix}
        \end{align*}
        
        The loss $L(2)$ would be
        \[
            L(1) = -\sum_{i=1}^2 o_i \log{\hat{o}_i} = 1 \times \log{(0.46454554)} + 0 \times \log{(0.53545446)} = 0.76669568
        \]
        
        As it should be expected, the loss $L$ decreases. The learning rate $\alpha$ might be too small. With code, we found that at the 1000-th iteration, the predicted $\bm{\hat{o}} \approx [0.999, 0.001]^\top$ with a loss $L \approx 0.00145$.
        In fact, if we choose $\alpha = 1$, we would arrive at the 99-th iteration to a predicted $\bm{\hat{o}} \approx [0.999, 0.001]^\top$ with a loss $L \approx 0.00142$.
    \end{itemize}

\end{homeworkProblem}

\printbibliography

\end{document}