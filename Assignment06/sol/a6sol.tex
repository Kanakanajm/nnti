\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{changepage}

\usetikzlibrary{automata,positioning}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{6}
\newcommand{\hmwkTitle}{Exercise Sheet \hmwkNumber}
\newcommand{\hmwkClass}{NNTI}
\newcommand{\hmwkTeam}{Team \#25}
\newcommand{\hmwkAuthorName}{\hmwkTeam: \\ Camilo Mart√≠nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de}
%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}


\begin{document}

\maketitle

\begin{homeworkProblem}{- Activation Functions}

    \subsection*{(a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        \begin{align*}
            1 - \sigma(x)   &= 1 - \frac{1}{1+e^{-x}}\\
                            &= \frac{1+e^{-x}-1}{1+e^{-x}}\\
                            &= \frac{e^{-x}}{1+e^{-x}}\\
                            &= \frac{\frac{e^{-x}}{e^{-x}}}{\frac{1}{e^{-x}}+ \frac{e^{-x}}{e^{-x}}}\\
                            &= \frac{1}{1+e^x}\\
                            &= \sigma(-x)
        \end{align*}
    \end{adjustwidth}

    \subsection*{(b)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    \begin{align*}
        tanh(x) &= \frac{sinh(x)}{cosh(x)}\\
                &= \frac{e^x-e{-x}}{e^x+e^{-x}}\\
                &= \frac{e^{2x}-1}{e^{2x}+1}\\
        2\sigma(2x)-1   &= \frac{2}{1+e^{-2x}} - 1\\
                        &= \frac{2-1-e^{-2x}}{1+e^{-2x}}\\
                        &= \frac{1-e^{-2x}}{1+e^{-2x}}\\
                        &= \frac{\frac{1}{e^{-2x}}-\frac{e^{-2x}}{e^{-2x}}}{\frac{1}{e^{-2x}}+\frac{e^{-2x}}{e^{-2x}}}\\
                        &= \frac{e^{2x}-1}{e^{2x}+1}
    \end{align*}
    \end{adjustwidth}

    \subsection*{(c)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        \begin{align*}
            \sigma'(x)  &= -(1+e^{-x})^{-2}(e^{-x})(-1)\\
                        &= (1+e^{-x})^{-2}(e^{-x})\\
                        &= \frac{e^{-x}}{(1+e^{-x})^2}\\
            \sigma(-x)\cdot\sigma(x)    &= \frac{1}{1+e^x}\cdot\frac{1}{1+e^{-x}}\\
                                        &= \frac{\frac{1}{e^x}}{\frac{1}{e^x}+\frac{e^x}{e^x}}\cdot\frac{1}{1+e^{-x}}\\
                                        &= \frac{e^{-x}}{1+e^{-x}}\cdot\frac{1}{1+e^{-x}}\\
                                        &= \frac{e^{-x}}{(1+e^{-x})^2}
        \end{align*}
    \end{adjustwidth}
    
    \subsection*{(d)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        \begin{align*}
            \frac{d}{dx}(2\sigma(2x)-1) &= 4\sigma'(2x)\\
                                        &= 4\sigma(-2x)\sigma(2x)\\
                                        &= 4(1-\sigma(2x))\sigma(2x)
        \end{align*}
    \end{adjustwidth}


\end{homeworkProblem}

\begin{homeworkProblem}{- Introduction to convexity}

    \subsection*{(a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        $f(x^\star)$ is a minimum so $\nabla_{x}f(x^\star) = 0$ so the First-order characterization becomes: $f(y) \ge f(x^\star)$
    \end{adjustwidth}

    \newcommand{\enormsqr}[1]{||#1||_2^2}
    \newcommand{\vecdot}[2]{\langle#1,#2\rangle}
    \subsection*{(b)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        We have $f(w)=\enormsqr{Xw-Y}$ so $\forall w,w' \in \mathrm{dom}(f)$
        \begin{align*}
            &f(\alpha w+(1-\alpha)w') - \alpha f(w) - (1-\alpha)f(w')\\
            &=\enormsqr{X(\alpha w+(1-\alpha)w')-Y}-\alpha\enormsqr{Xw-Y}-(1-\alpha)\enormsqr{Xw'-Y}\\
            &=\enormsqr{X(\alpha w+(1-\alpha)w')} - 2\vecdot{X(\alpha w+(1-\alpha)w')}{Y} + \enormsqr{Y} - \alpha(\enormsqr{Xw} - 2\vecdot{Xw}{Y} + \enormsqr{Y}) - \dots \\ 
            & \quad \, (1-\alpha)(\enormsqr{Xw'} - 2\vecdot{Xw'}{Y} + \enormsqr{Y})
        \end{align*}

        All terms with $\enormsqr{Y}$ cancel because $\enormsqr{Y} - \alpha\enormsqr{Y} - (1-\alpha)\enormsqr{Y} = 0$ and all terms like $\vecdot{..}{Y}$ cancel because
        \begin{align*}
            & -2\vecdot{X(\alpha w + (1-\alpha)w')}{Y} + 2\alpha\vecdot{Xw}{Y} + 2(1-\alpha)\vecdot{Xw'}{Y}\\
            &= 2\vecdot{-X(\alpha w + (1-\alpha)w' + \alpha Xw + (1-\alpha)Xw')}{Y}\\
            &= 2\vecdot{0}{Y}\\
            &= 0\\
        \end{align*}

        Now we have only $\enormsqr{X(\alpha w+(1-\alpha)w')} - \alpha\enormsqr{Xw} - (1-\alpha)\enormsqr{Xw'}$ which becomes
        \begin{align*}
           &= \enormsqr{\alpha Xw} + \enormsqr{(1-\alpha)Xw'} + 2\vecdot{\alpha Xw}{(1-\alpha Xw')} - \alpha\enormsqr{Xw} - (1-\alpha)\enormsqr{Xw'}\\
           &= \alpha^2\enormsqr{Xw} + (1-\alpha)^2\enormsqr{Xw'} + 2\vecdot{\alpha Xw}{(1-\alpha Xw')} - \alpha\enormsqr{Xw} - (1-\alpha)\enormsqr{Xw'}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2-2(\alpha^2-\alpha)\vecdot{Xw}{Xw'} + 2\vecdot{\alpha Xw}{(1-\alpha)Xw'}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2+2(-\alpha)(1 - \alpha)\vecdot{Xw}{Xw'} + 2\vecdot{\alpha Xw}{(1-\alpha)Xw'}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2+2\vecdot{(-\alpha)Xw}{(1 - \alpha)Xw'} + 2\vecdot{\alpha Xw}{(1-\alpha)Xw'}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2+2\vecdot{(-\alpha)Xw + \alpha Xw}{(1-\alpha Xw')}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2+2\vecdot{0}{(1-\alpha Xw')}\\
           &= (\alpha^2-\alpha)(\enormsqr{Xw}-\enormsqr{Xw'})^2+0\\
           &= \alpha(\alpha -1)(\enormsqr{Xw}-\enormsqr{Xw'})^2+0 \le 0 \quad \text{because $\alpha \le 1$}
        \end{align*}
        Thus $f(\alpha w+(1-\alpha)w') \le \alpha f(w) + (1-\alpha)f(w')$
    \end{adjustwidth}


    \subsection*{(c)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
        No, Neural Networks are not convex functions in general. In the lecture, it was told that for each loss function of a neural network
        there could be multiple minima, i.e. multiple models. There could also be saddle points which is a minima with respect to one variable
        and a maxima with respect to another.
    \end{adjustwidth}


\end{homeworkProblem}

\begin{homeworkProblem}{- General Questions}

    \subsection*{a)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    They introduce non-linearity, which in turns helps learn more complex functions. Otherwise, our Neural Networks would just be multiplication of weight matrices, which obviously can be represented by a single weight matrix. 
    \end{adjustwidth}

    \subsection*{b)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    Faster to compute and numerically stable, since it does not involve complex mathematical operations that may under- or overflow, such as exponentials in the sigmoid function or a geometrical one like tanh.
    \end{adjustwidth}

    \subsection*{c)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    When the input of the softmax function $z_i$ is big or small, the softmax function could overflow or underflow respectively, because of the exponential.
    We can shift the input value: instead of $z_i$, we use $z_i - \underset{j}{\max{ }}{z_j}$.
    \end{adjustwidth}

    \subsection*{d)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    Because it involves calculating the inverse of a matrix, which is an extremely costly operation with a time complexity of $O(n^3)$.
    \end{adjustwidth}

    \subsection*{e)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    No, in general any method that numerically attempts to find a minimum only guarantees a local minimum. That is, it may very well exist another minimum that could be found with a different initialization of parameters. A true minimum could theoretically be found, if we knew the mathematical formula of the loss function and that is not feasible with Neural Networks.
    \end{adjustwidth}

    \subsection*{f)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    In general, for a Neural Network with a simple loss function (that is, one that does not have any additional techniques such as regularization), weight space symmetry would hinder the optimization process. In such a case, we would have many equivalent local minimas. Nevertheless, the loss surface would also be prone to saddle points, which are points where the gradient is zero but are neither maxima nor minima. These points can slow down typical optimization algorithms such as Gradient-Descent or even get stuck in them.
    \end{adjustwidth}

    \subsection*{g)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    When the loss function we try to optimize is a simple function like a convex parabola, Newton's method can find a minima in one step
    while the normal gradient decent may take multiple steps depending on the starting position and learning rate.
    \end{adjustwidth}

    \subsection*{h)} 
    \vspace*{-2em}
    \begin{adjustwidth}{2.5em}{0pt}
    It cannot compute the std and the mean.
    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{- BatchNorm}
    The paper centers around a technique called Batch Normalization, which consists of introducing additional network layers that control the first two moments (mean and variance) of the distributions of layer inputs. The authors aim to show that the practical success of using Batch Normalization in a Neural Network does not come from reducing "Internal Covariate Shift" (which refers to the continual change in the distribution of layer inputs caused by updates to the preceding layers and is conjectured that it negatively impacts training). Instead, the authors argue that its benefits, mainly faster training and convergence, come from the fact that Batch Normalization smoothens the optimization landscape, effectively reparametrizing the underlying optimization problem to make it more stable and smooth. This implies that the gradients used in
    training are more predictive and well-behaved, which enables faster and more effective optimization. In simpler words, their findings showed that the primary benefit of BatchNorm lies in facilitating smoother gradients rather than stabilizing layer inputs.

\end{homeworkProblem}

\begin{homeworkProblem}{- Forward Pass for a Fully-Connected Neural Network}

Given the weights of the simple Feed Forward Neural Network (FFNN) with one hidden layer, defined in the problem:

\[
    W^{(1)} = 
    \begin{bmatrix}
    -0.2 & 0.9 & 0.4 \\
    -0.1 & 0.3 & 0.4 \\
    0.2 & 0.5 & -0.7 \\
    0.2 & -0.5 & 0.5
    \end{bmatrix}
    \quad
    W^{(2)} = 
    \begin{bmatrix}
    0.6 & -0.2 \\
    -0.1 & 0.8 \\
    -0.5 & -0.3
    \end{bmatrix}
\]

where $W^{(1)}$ is the weight matrix that acts upon the $1^{\text{st}}$ layer (input layer) and $W^{(2)}$, the weight matrix that acts upon the activations of the $2^{\text{nd}}$ layer (hidden layer). \\

The forward pass in this Network would look like this:
\[
    \bm{h} = 
    \begin{bmatrix}
    h_1 \\
    h_2 \\
    h_3 
    \end{bmatrix}
    = \mathtt{ReLU}\left(\left[W^{(1)}\right]^\top\bm{x}\right) = 
    \mathtt{ReLU}\left(
        \begin{bmatrix}
        -0.2 & -0.1 & 0.2 & 0.2 \\
        0.9 & 0.3 & 0.5 & -0.5 \\
        0.4 & 0.4 & -0.7 & 0.5
        \end{bmatrix}
        \begin{bmatrix}
        3 \\
        1 \\
        -1 \\
        2 
        \end{bmatrix}
    \right) =
    \mathtt{ReLU}\left(
        \begin{bmatrix}
        -0.5 \\
        1.5 \\
        3.3
        \end{bmatrix}
    \right) =
    \begin{bmatrix}
    0 \\
    1.5 \\
    3.3
    \end{bmatrix}
\]

\[
    \bm{o} = 
    \begin{bmatrix}
    o_1 \\
    o_2 
    \end{bmatrix}
    = \mathtt{Softmax}\left(\left[W^{(2)}\right]^\top\bm{h}\right) = 
    \mathtt{Softmax}\left(
        \begin{bmatrix}
        0.6 & -0.1 & -0.5 \\
        -0.2 & 0.8 & -0.3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        1.5 \\
        3.3
        \end{bmatrix}
    \right) =
    \mathtt{Softmax}\left(
        \begin{bmatrix}
        -2.1 \\
        0.31
        \end{bmatrix}
    \right) =
    \begin{bmatrix}
        0.08241332 \\ 
        0.91758668
    \end{bmatrix}
\]

\end{homeworkProblem}

\begin{homeworkProblem}{- Clustering Neural Networks}
See attached files, the python code that was ran in the cluster is in the file $\mathtt{nnti\_assignment\_6.py}$. 
The folder $\mathtt{/code/models}$ has the saved trained models per epoch. The accuracy of these models can be checked out with the same provided code by commenting out the line that calls the function $\mathtt{perform\_training()}$. On the other hand, the log $\mathtt{/code/logs/nnti\_assignment\_6.35457.0.out}$ shows the console information produced by the code ran on the cluster. In particular, the last lines contain the information of the best trained model: \\

Best found model: model\_20231210\_180121\_epoch\_9 \\
Training Accuracy: 91.92\%, \\
Validation Accuracy: 89.82\%, \\
Test Accuracy: 89.14\%

\end{homeworkProblem}

\end{document}